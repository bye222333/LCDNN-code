{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aae377e0",
      "metadata": {
        "id": "aae377e0"
      },
      "source": [
        "# LCDNN — debug & paper-aligned synthetic verification\n",
        "\n",
        "This notebook is meant to **diagnose why Precision/Recall/F1/ARI can collapse to ~0** and to run a **paper-aligned synthetic generator** (Algorithm “Synthetic NoNs generator” in your TeX) so you can quickly verify that the full LCDNN pipeline behaves as expected.\n",
        "\n",
        "What it does:\n",
        "\n",
        "- Adds **unit tests** for the metric code (so we can rule out evaluation bugs).\n",
        "- Uses a **paper-aligned synthetic generator** (p=48, m=8, k_int=2, r=2, n=2000, Laplace noise, spillover scales (0.2,0.5,1.0,0.5,0.2)).\n",
        "- Runs LCDNN on **one target (default: N3, strongest spillover)** for speed, and reports **ARI + directed-edge Precision/Recall/F1**.\n",
        "- Provides an **orientation sanity check** (if `A_hat.T` matches truth much better than `A_hat`, you likely have a from/to convention mismatch).\n",
        "\n",
        "> Tip: Start with `MODE=\"FAST\"` to confirm the code path and non-zero scores, then switch to `MODE=\"PAPER\"` for closer reproduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ae3066",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07ae3066",
        "outputId": "07da9283-3e2d-4745-cb6d-276cbc62adbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy 2.0.2\n",
            "scipy 1.16.3\n",
            "pandas 2.2.2\n",
            "sklearn 1.6.1\n",
            "lingam 1.12.2\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------\n",
        "# Dependencies (install only if missing)\n",
        "# ----------------------------\n",
        "# This cell is designed to be fast on reruns:\n",
        "# it only installs packages that are not already importable.\n",
        "\n",
        "AUTO_INSTALL = True\n",
        "\n",
        "if AUTO_INSTALL:\n",
        "    import sys, subprocess, importlib\n",
        "    from typing import Optional\n",
        "\n",
        "    def ensure(pkg: str, import_name: Optional[str] = None):\n",
        "        name = import_name or pkg\n",
        "        try:\n",
        "            importlib.import_module(name)\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "    ensure(\"numpy\")\n",
        "    ensure(\"scipy\")\n",
        "    ensure(\"pandas\")\n",
        "    ensure(\"scikit-learn\", \"sklearn\")\n",
        "    ensure(\"tqdm\")\n",
        "    ensure(\"lingam\")\n",
        "\n",
        "import numpy as np, pandas as pd, scipy\n",
        "import sklearn\n",
        "import lingam\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"scipy\", scipy.__version__)\n",
        "print(\"pandas\", pd.__version__)\n",
        "print(\"sklearn\", sklearn.__version__)\n",
        "print(\"lingam\", lingam.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfffb05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dfffb05",
        "outputId": "f3ef8f76-b1fb-4f09-ea04-649d5084fc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Done. LaTeX tables written to: ./tables_generated/\n",
            "\n",
            "Generated files (key ones):\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Tuple, Optional, Set, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from scipy import linalg\n",
        "from scipy.stats import gamma\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# 1) Reproducibility\n",
        "# =========================\n",
        "def set_all_seeds(seed: int) -> None:\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def standardize(X: np.ndarray) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sd = X.std(axis=0, keepdims=True, ddof=1)\n",
        "    sd[sd < 1e-12] = 1.0\n",
        "    return (X - mu) / sd\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) HSIC independence test (Gamma approximation; fast, no perms)\n",
        "# ============================================================\n",
        "# This follows the widely used HSIC Gamma-approx test (Gretton et al.),\n",
        "# with a median heuristic bandwidth. It is fast enough for large sweeps.\n",
        "\n",
        "def _pairwise_sq_dists(X: np.ndarray) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    G = np.sum(X * X, axis=1, keepdims=True)\n",
        "    return G + G.T - 2.0 * (X @ X.T)\n",
        "\n",
        "def _median_sigma(X: np.ndarray) -> float:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    n = X.shape[0]\n",
        "    D = _pairwise_sq_dists(X)\n",
        "    tri = D[np.triu_indices(n, k=1)]\n",
        "    tri = tri[tri > 0]\n",
        "    if tri.size == 0:\n",
        "        return 1.0\n",
        "    return float(np.sqrt(0.5 * np.median(tri)))\n",
        "\n",
        "def _rbf_kernel(X: np.ndarray, sigma: float) -> np.ndarray:\n",
        "    D = _pairwise_sq_dists(X)\n",
        "    K = np.exp(-D / (2.0 * sigma * sigma))\n",
        "    return K\n",
        "\n",
        "\n",
        "def _center_gram(K: np.ndarray) -> np.ndarray:\n",
        "    K = np.asarray(K, dtype=float)\n",
        "    mean_col = K.mean(axis=0, keepdims=True)\n",
        "    mean_row = K.mean(axis=1, keepdims=True)\n",
        "    mean_all = float(K.mean())\n",
        "    return K - mean_col - mean_row + mean_all\n",
        "\n",
        "def _median_sigma_subsample(X: np.ndarray, max_points: int = 200, seed: int = 0) -> float:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    n = X.shape[0]\n",
        "    if max_points is not None and n > max_points:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        idx = rng.choice(n, size=max_points, replace=False)\n",
        "        X = X[idx]\n",
        "    return _median_sigma(X)\n",
        "\n",
        "def hsic_statistic(X: np.ndarray, Y: np.ndarray, *, max_sigma_points: int = 200, seed: int = 0) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    Y = np.asarray(Y, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    if Y.ndim == 1:\n",
        "        Y = Y.reshape(-1, 1)\n",
        "    n = X.shape[0]\n",
        "    assert Y.shape[0] == n,\n",
        "\n",
        "    sig_x = _median_sigma_subsample(X, max_points=max_sigma_points, seed=seed)\n",
        "    sig_y = _median_sigma_subsample(Y, max_points=max_sigma_points, seed=seed + 1)\n",
        "\n",
        "    K = _rbf_kernel(X, sig_x)\n",
        "    L = _rbf_kernel(Y, sig_y)\n",
        "\n",
        "    Kc = _center_gram(K)\n",
        "    Lc = _center_gram(L)\n",
        "\n",
        "    # Biased HSIC estimator (constant factors cancel in permutation tests)\n",
        "    hsic = float(np.sum(Kc * Lc) / (n * n))\n",
        "    return hsic, K, L, Kc\n",
        "\n",
        "def hsic_gamma_pvalue(X: np.ndarray, Y: np.ndarray, *, max_sigma_points: int = 200, seed: int = 0) -> Tuple[float, float]:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    Y = np.asarray(Y, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    if Y.ndim == 1:\n",
        "        Y = Y.reshape(-1, 1)\n",
        "    n = X.shape[0]\n",
        "    assert Y.shape[0] == n,\n",
        "\n",
        "    test_stat, K, L, Kc = hsic_statistic(X, Y, max_sigma_points=max_sigma_points, seed=seed)\n",
        "    Lc = _center_gram(L)\n",
        "\n",
        "    # Variance estimate (same style as common reference implementations)\n",
        "    var_hsic = (Kc * Lc / 6.0) ** 2\n",
        "    var_hsic = float((np.sum(var_hsic) - np.trace(var_hsic)) / (n * (n - 1)))\n",
        "    if n > 5:\n",
        "        var_hsic = var_hsic * 72.0 * (n - 4) * (n - 5) / (n * (n - 1) * (n - 2) * (n - 3))\n",
        "    else:\n",
        "        var_hsic = max(var_hsic, 1e-12)\n",
        "\n",
        "    K_off = K - np.diag(np.diag(K))\n",
        "    L_off = L - np.diag(np.diag(L))\n",
        "    one = np.ones((n, 1))\n",
        "    mu_x = float((one.T @ K_off @ one) / (n * (n - 1)))\n",
        "    mu_y = float((one.T @ L_off @ one) / (n * (n - 1)))\n",
        "    m_hsic = float((1.0 + mu_x * mu_y - mu_x - mu_y) / n)\n",
        "\n",
        "    # Guards\n",
        "    if (not np.isfinite(var_hsic)) or (not np.isfinite(m_hsic)) or var_hsic <= 1e-12 or m_hsic <= 1e-12:\n",
        "        return test_stat, 1.0\n",
        "\n",
        "    alpha = (m_hsic ** 2) / var_hsic\n",
        "    beta = var_hsic / m_hsic  # scale\n",
        "    pval = float(1.0 - gamma.cdf(test_stat, alpha, scale=beta))\n",
        "    pval = max(min(pval, 1.0), 0.0)\n",
        "    return test_stat, pval\n",
        "\n",
        "def hsic_perm_pvalue(X: np.ndarray, Y: np.ndarray, *, n_perm: int = 200, max_sigma_points: int = 200, seed: int = 0) -> Tuple[float, float]:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    Y = np.asarray(Y, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    if Y.ndim == 1:\n",
        "        Y = Y.reshape(-1, 1)\n",
        "    n = X.shape[0]\n",
        "    assert Y.shape[0] == n,\n",
        "\n",
        "    stat_obs, K, L, Kc = hsic_statistic(X, Y, max_sigma_points=max_sigma_points, seed=seed)\n",
        "    # Pre-center K once; permute L\n",
        "    rng = np.random.default_rng(seed + 12345)\n",
        "    stats = []\n",
        "    for _ in range(int(n_perm)):\n",
        "        perm = rng.permutation(n)\n",
        "        Lp = L[np.ix_(perm, perm)]\n",
        "        Lpc = _center_gram(Lp)\n",
        "        stats.append(float(np.sum(Kc * Lpc) / (n * n)))\n",
        "    stats = np.asarray(stats, dtype=float)\n",
        "    pval = float((np.sum(stats >= stat_obs) + 1.0) / (len(stats) + 1.0))\n",
        "    return stat_obs, pval\n",
        "\n",
        "def hsic_pvalue(\n",
        "    X: np.ndarray,\n",
        "    Y: np.ndarray,\n",
        "    *,\n",
        "    method: str = \"perm\",\n",
        "    n_perm: int = 200,\n",
        "    max_sigma_points: int = 200,\n",
        "    seed: int = 0,\n",
        ") -> Tuple[float, float]:\n",
        "    method = str(method).lower()\n",
        "    if method in [\"perm\", \"permutation\"]:\n",
        "        return hsic_perm_pvalue(X, Y, n_perm=n_perm, max_sigma_points=max_sigma_points, seed=seed)\n",
        "    if method in [\"gamma\", \"gaussian\", \"approx\"]:\n",
        "        return hsic_gamma_pvalue(X, Y, max_sigma_points=max_sigma_points, seed=seed)\n",
        "    raise ValueError(f\"Unknown HSIC method: {method}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# HSIC speed/compatibility shim\n",
        "# ------------------------------------------------------------\n",
        "_HSIC_PVALUE_ORIG = hsic_pvalue\n",
        "\n",
        "def enable_fast_hsic(max_sigma_points: int = 200, seed: int = 0) -> None:\n",
        "    global hsic_pvalue\n",
        "\n",
        "    def _wrapped(\n",
        "        X: np.ndarray,\n",
        "        Y: np.ndarray,\n",
        "        *,\n",
        "        method: str = \"perm\",\n",
        "        n_perm: int = 200,\n",
        "        max_sigma_points: int = max_sigma_points,\n",
        "        seed: int = seed,\n",
        "    ) -> Tuple[float, float]:\n",
        "        return _HSIC_PVALUE_ORIG(\n",
        "            X,\n",
        "            Y,\n",
        "            method=method,\n",
        "            n_perm=n_perm,\n",
        "            max_sigma_points=max_sigma_points,\n",
        "            seed=seed,\n",
        "        )\n",
        "\n",
        "    hsic_pvalue = _wrapped\n",
        "\n",
        "def disable_fast_hsic() -> None:\n",
        "    global hsic_pvalue\n",
        "    hsic_pvalue = _HSIC_PVALUE_ORIG\n",
        "\n",
        "# ============================================================\n",
        "# 3) DL-GIN (Phase I): nullspace projection + HSIC\n",
        "# ============================================================\n",
        "\n",
        "def estimate_rank_by_relative_threshold(svals: np.ndarray, rel_tol: float) -> int:\n",
        "    svals = np.asarray(svals, dtype=float)\n",
        "    if svals.size == 0:\n",
        "        return 0\n",
        "    mx = float(svals[0])\n",
        "    if mx <= 1e-12:\n",
        "        return 0\n",
        "    return int(np.sum(svals >= rel_tol * mx))\n",
        "\n",
        "def estimate_rank_by_noise_edge(svals: np.ndarray, n: int, n_rows: int, n_cols: int, k: float = 1.5) -> int:\n",
        "\n",
        "    svals = np.asarray(svals, dtype=float)\n",
        "    if svals.size == 0:\n",
        "        return 0\n",
        "    tau = float(k * (np.sqrt(n_rows) + np.sqrt(n_cols)) / np.sqrt(max(n - 1, 1)))\n",
        "    return int(np.sum(svals > tau))\n",
        "\n",
        "def left_nullspace_basis(\n",
        "    M: np.ndarray,\n",
        "    *,\n",
        "    n: int,\n",
        "    method: str = \"noise\",\n",
        "    rel_tol: float = 0.3,\n",
        "    noise_k: float = 1.5,\n",
        ") -> np.ndarray:\n",
        "\n",
        "    U, S, _ = np.linalg.svd(M, full_matrices=True)\n",
        "    method = str(method).lower()\n",
        "    if method in [\"noise\", \"noise_edge\", \"rmt\"]:\n",
        "        rank = estimate_rank_by_noise_edge(S, n=n, n_rows=M.shape[0], n_cols=M.shape[1], k=noise_k)\n",
        "    elif method in [\"relative\", \"rel\"]:\n",
        "        rank = estimate_rank_by_relative_threshold(S, rel_tol=rel_tol)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown rank method: {method}\")\n",
        "    rank = min(rank, M.shape[0])\n",
        "    return U[:, rank:]  # left nullspace basis\n",
        "\n",
        "def dl_gin_pvalue(\n",
        "    X: np.ndarray,\n",
        "    C: Iterable[int],\n",
        "    *,\n",
        "    alpha_hsic: float = 0.05,\n",
        "    rank_method: str = \"noise\",\n",
        "    rel_tol: float = 0.3,\n",
        "    noise_k: float = 1.5,\n",
        "    hsic_method: str = \"perm\",\n",
        "    hsic_n_perm: int = 200,\n",
        "    hsic_max_sigma_points: int = 200,\n",
        "    seed: int = 0,\n",
        ") -> float:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n, p = X.shape\n",
        "    C = sorted(set(C))\n",
        "    R = [j for j in range(p) if j not in set(C)]\n",
        "    if len(R) == 0:\n",
        "        return 0.0  # do not merge into a single cluster; R empty makes the test vacuous\n",
        "\n",
        "    Xc = X[:, C]\n",
        "    Xr = X[:, R]\n",
        "    Xc0 = Xc - Xc.mean(axis=0, keepdims=True)\n",
        "    Xr0 = Xr - Xr.mean(axis=0, keepdims=True)\n",
        "\n",
        "    # Sample cross-covariance (ddof=1 scaling matches most numerical practice; paper uses 1/n)\n",
        "    Sigma_cr = (Xc0.T @ Xr0) / max(n - 1, 1)\n",
        "\n",
        "    W = left_nullspace_basis(Sigma_cr, n=n, method=rank_method, rel_tol=rel_tol, noise_k=noise_k)\n",
        "    if W.shape[1] == 0:\n",
        "        # no usable projection -> cannot certify \"GIN\", so reject merge\n",
        "        return 0.0\n",
        "\n",
        "    Z = Xc0 @ W  # n x d\n",
        "    _, pval = hsic_pvalue(\n",
        "        Z,\n",
        "        Xr0,\n",
        "        method=hsic_method,\n",
        "        n_perm=hsic_n_perm,\n",
        "        max_sigma_points=hsic_max_sigma_points,\n",
        "        seed=seed,\n",
        "    )\n",
        "    return float(pval)\n",
        "\n",
        "def agglomerative_gin_clustering(\n",
        "    X: np.ndarray,\n",
        "    *,\n",
        "    alpha_gin: float,\n",
        "    rank_method: str = \"noise\",\n",
        "    rel_tol: float = 0.3,\n",
        "    noise_k: float = 1.5,\n",
        "    hsic_method: str = \"perm\",\n",
        "    hsic_n_perm: int = 200,\n",
        "    hsic_max_sigma_points: int = 200,\n",
        "    max_merges: Optional[int] = None,\n",
        "    max_pair_checks: Optional[int] = None,\n",
        "    seed: int = 0,\n",
        ") -> List[Set[int]]:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = X.shape[1]\n",
        "    clusters: List[Set[int]] = [{i} for i in range(p)]\n",
        "    merges = 0\n",
        "\n",
        "    while True:\n",
        "        merged = False\n",
        "\n",
        "        clusters_sorted = sorted(clusters, key=lambda s: (len(s), min(s)))\n",
        "\n",
        "        pairs = []\n",
        "        for a in range(len(clusters_sorted)):\n",
        "            for b in range(a + 1, len(clusters_sorted)):\n",
        "                Ca = clusters_sorted[a]\n",
        "                Cb = clusters_sorted[b]\n",
        "                pairs.append((len(Ca) + len(Cb), min(Ca), min(Cb), Ca, Cb))\n",
        "        pairs.sort(key=lambda t: (t[0], t[1], t[2]))\n",
        "\n",
        "        checks = 0\n",
        "        for _, _, _, Ca, Cb in pairs:\n",
        "            C = Ca | Cb\n",
        "            if len(C) == p:\n",
        "                continue  # never merge into a single cluster (R would be empty)\n",
        "            pval = dl_gin_pvalue(\n",
        "                X,\n",
        "                C,\n",
        "                alpha_hsic=alpha_gin,\n",
        "                rank_method=rank_method,\n",
        "                rel_tol=rel_tol,\n",
        "                noise_k=noise_k,\n",
        "                hsic_method=hsic_method,\n",
        "                hsic_n_perm=hsic_n_perm,\n",
        "                hsic_max_sigma_points=hsic_max_sigma_points,\n",
        "                seed=seed + merges * 10000 + checks,\n",
        "            )\n",
        "            checks += 1\n",
        "            if pval > alpha_gin:\n",
        "                new_clusters = [S for S in clusters if S != Ca and S != Cb]\n",
        "                new_clusters.append(C)\n",
        "                clusters = new_clusters\n",
        "                merged = True\n",
        "                merges += 1\n",
        "                break\n",
        "\n",
        "            if (max_pair_checks is not None) and (checks >= max_pair_checks):\n",
        "                break\n",
        "\n",
        "        if not merged:\n",
        "            break\n",
        "        if (max_merges is not None) and (merges >= max_merges):\n",
        "            break\n",
        "\n",
        "    return [set(sorted(list(c))) for c in clusters]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Phase II: CA-RCD = (RCD baseline) + (cluster-aware pruning)\n",
        "# ============================================================\n",
        "\n",
        "def ols_fit_and_residual(y: np.ndarray, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    n = y.shape[0]\n",
        "    if X.size == 0:\n",
        "        resid = y - y.mean()\n",
        "        beta = np.array([y.mean()])  # intercept only\n",
        "        return beta, resid\n",
        "\n",
        "    # add intercept\n",
        "    X1 = np.column_stack([X, np.ones(n)])\n",
        "    # solve least squares\n",
        "    beta, *_ = np.linalg.lstsq(X1, y, rcond=None)\n",
        "    resid = y - X1 @ beta\n",
        "    return beta, resid\n",
        "\n",
        "def refit_linear_sem(X: np.ndarray, parents: List[List[int]]) -> np.ndarray:\n",
        "    n, p = X.shape\n",
        "    B = np.zeros((p, p), dtype=float)\n",
        "    for j in range(p):\n",
        "        Pj = parents[j]\n",
        "        if len(Pj) == 0:\n",
        "            continue\n",
        "        beta, _ = ols_fit_and_residual(X[:, j], X[:, Pj])\n",
        "        coef = beta[:-1]  # exclude intercept\n",
        "        for k, i in enumerate(Pj):\n",
        "            B[i, j] = coef[k]\n",
        "    return B\n",
        "\n",
        "def compute_residuals_from_B(X: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    n, p = X.shape\n",
        "    R = np.zeros_like(X)\n",
        "    for j in range(p):\n",
        "        Pj = list(np.where(np.abs(B[:, j]) > 0)[0])\n",
        "        if len(Pj) == 0:\n",
        "            _, resid = ols_fit_and_residual(X[:, j], np.empty((n, 0)))\n",
        "        else:\n",
        "            _, resid = ols_fit_and_residual(X[:, j], X[:, Pj])\n",
        "        R[:, j] = resid\n",
        "    return R\n",
        "\n",
        "def run_direct_lingam(X: np.ndarray) -> np.ndarray:\n",
        "    from lingam import DirectLiNGAM\n",
        "    model = DirectLiNGAM()\n",
        "    model.fit(X)\n",
        "\n",
        "    W_to_from = np.asarray(model.adjacency_matrix_, dtype=float)\n",
        "    W_from_to = W_to_from.T  # convert to our convention\n",
        "\n",
        "    A = (np.abs(W_from_to) > 1e-6).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A\n",
        "\n",
        "def parse_lingam_rcd_output(adj: np.ndarray, w_threshold: float = 1e-8):\n",
        "    adj = np.asarray(adj, dtype=float)\n",
        "    p = adj.shape[0]\n",
        "\n",
        "    conf_pairs = set()\n",
        "    B = np.zeros((p, p), dtype=float)\n",
        "\n",
        "    for to in range(p):\n",
        "        for frm in range(p):\n",
        "            if to == frm:\n",
        "                continue\n",
        "\n",
        "            a = adj[to, frm]     # frm -> to (lingam convention)\n",
        "            b = adj[frm, to]     # to  -> frm (lingam convention)\n",
        "\n",
        "            # mark conf/unknown if either direction is nan\n",
        "            if np.isnan(a) or np.isnan(b):\n",
        "                conf_pairs.add((min(frm, to), max(frm, to)))\n",
        "\n",
        "            # keep directed edge if finite\n",
        "            if np.isfinite(a) and abs(a) > w_threshold:\n",
        "                B[frm, to] = a    # convert to our convention (from,to)\n",
        "\n",
        "    return B, conf_pairs\n",
        "\n",
        "\n",
        "def prune_edges_by_hsic(\n",
        "    X: np.ndarray,\n",
        "    B: np.ndarray,\n",
        "    *,\n",
        "    alpha: float = 0.05,\n",
        "    hsic_method: str = \"perm\",\n",
        "    hsic_n_perm: int = 200,\n",
        "    hsic_max_sigma_points: int = 200,\n",
        "    max_rounds: int = 1,\n",
        "    seed: int = 0,\n",
        ") -> np.ndarray:\n",
        "\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n, p = X.shape\n",
        "    Bp = B.copy()\n",
        "\n",
        "    for rr in range(int(max_rounds)):\n",
        "        changed = False\n",
        "        for j in range(p):\n",
        "            parents = list(np.where(np.abs(Bp[:, j]) > 0)[0])\n",
        "            if len(parents) == 0:\n",
        "                continue\n",
        "            for i in parents:\n",
        "                P_wo = [k for k in parents if k != i]\n",
        "                if len(P_wo) == 0:\n",
        "                    _, resid = ols_fit_and_residual(X[:, j], np.empty((n, 0)))\n",
        "                else:\n",
        "                    _, resid = ols_fit_and_residual(X[:, j], X[:, P_wo])\n",
        "                _, pval = hsic_pvalue(\n",
        "                    resid,\n",
        "                    X[:, i],\n",
        "                    method=hsic_method,\n",
        "                    n_perm=hsic_n_perm,\n",
        "                    max_sigma_points=hsic_max_sigma_points,\n",
        "                    seed=seed + 100000 * rr + 1000 * j + i,\n",
        "                )\n",
        "                if pval > alpha:\n",
        "                    Bp[i, j] = 0.0\n",
        "                    changed = True\n",
        "        if not changed:\n",
        "            break\n",
        "    return Bp\n",
        "\n",
        "def _make_rcd(**kwargs):\n",
        "    import inspect\n",
        "    from lingam import RCD\n",
        "    sig = inspect.signature(RCD)\n",
        "    filtered = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
        "    return RCD(**filtered)\n",
        "\n",
        "def ca_rcd(\n",
        "    X: np.ndarray,\n",
        "    clusters: List[Set[int]],\n",
        "    *,\n",
        "    # RCD hyperparameters (IMPORTANT: defaults in lingam are often too restrictive)\n",
        "    rcd_max_explanatory_num: Optional[int] = None,\n",
        "    rcd_cor_alpha: float = 0.05,\n",
        "    rcd_ind_alpha: float = 0.05,\n",
        "    rcd_shapiro_alpha: float = 0.05,\n",
        "    rcd_MLHSICR: bool = False,\n",
        "    rcd_bw_method: str = \"mdbs\",\n",
        "    rcd_independence: str = \"hsic\",\n",
        "    # Post-processing\n",
        "    w_threshold: float = 1e-6,\n",
        "    do_prune: bool = False,\n",
        "    prune_alpha: float = 0.05,\n",
        "    prune_hsic_method: str = \"perm\",\n",
        "    prune_hsic_n_perm: int = 200,\n",
        "    prune_hsic_max_sigma_points: int = 200,\n",
        "    prune_max_rounds: int = 1,\n",
        "    verbose: bool = False,\n",
        "    seed: int = 0,\n",
        ") -> Tuple[np.ndarray, Set[Tuple[int, int]]]:\n",
        "\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n, p = X.shape\n",
        "\n",
        "    if rcd_max_explanatory_num is None:\n",
        "        # Practical default: allow moderate in-degree but keep runtime reasonable\n",
        "        rcd_max_explanatory_num = min(6, p - 1)\n",
        "\n",
        "    model = _make_rcd(\n",
        "        max_explanatory_num=int(rcd_max_explanatory_num),\n",
        "        cor_alpha=float(rcd_cor_alpha),\n",
        "        ind_alpha=float(rcd_ind_alpha),\n",
        "        shapiro_alpha=float(rcd_shapiro_alpha),\n",
        "        MLHSICR=bool(rcd_MLHSICR),\n",
        "        bw_method=str(rcd_bw_method),\n",
        "        independence=str(rcd_independence),\n",
        "    )\n",
        "    model.fit(X)\n",
        "\n",
        "    B_init, conf_pairs = parse_lingam_rcd_output(model.adjacency_matrix_, w_threshold=w_threshold)\n",
        "\n",
        "    # Cluster map\n",
        "    cl_of = np.empty(p, dtype=int)\n",
        "    for ci, C in enumerate(clusters):\n",
        "        for v in C:\n",
        "            cl_of[v] = ci\n",
        "\n",
        "    # Remove within-cluster directed edges (forbidden)\n",
        "    for i in range(p):\n",
        "        for j in range(p):\n",
        "            if i != j and cl_of[i] == cl_of[j]:\n",
        "                B_init[i, j] = 0.0\n",
        "\n",
        "    # Remove within-cluster conf pairs (local latents explain these; we only keep cross-cluster)\n",
        "    conf_pairs = {pair for pair in conf_pairs if cl_of[pair[0]] != cl_of[pair[1]]}\n",
        "\n",
        "    # Optional extra pruning (can hurt recall if too aggressive)\n",
        "    B_work = B_init\n",
        "    if do_prune and prune_max_rounds > 0:\n",
        "        B_work = prune_edges_by_hsic(\n",
        "            X, B_work,\n",
        "            alpha=prune_alpha,\n",
        "            hsic_method=prune_hsic_method,\n",
        "            hsic_n_perm=prune_hsic_n_perm,\n",
        "            hsic_max_sigma_points=prune_hsic_max_sigma_points,\n",
        "            max_rounds=prune_max_rounds,\n",
        "            seed=seed,\n",
        "        )\n",
        "\n",
        "    # Refit by OLS given parent sets\n",
        "    parents = [list(np.where(np.abs(B_work[:, j]) > 0)[0]) for j in range(p)]\n",
        "    B_refit = refit_linear_sem(X, parents)\n",
        "\n",
        "    if verbose:\n",
        "        nnz = int(np.sum(np.abs(B_refit) > 0))\n",
        "        print(f\"CA-RCD: directed edges kept = {nnz}, confounded pairs kept = {len(conf_pairs)} (max_explanatory_num={rcd_max_explanatory_num})\")\n",
        "\n",
        "    return B_refit, conf_pairs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) Phase III: Typing via inter-span estimation + projection\n",
        "# ============================================================\n",
        "\n",
        "def estimate_inter_span_from_residuals(\n",
        "    R: np.ndarray,\n",
        "    clusters: List[Set[int]],\n",
        "    *,\n",
        "    max_rank: int = 10,\n",
        "    eigengap_min_rel: float = 0.05,\n",
        "    eigengap_min_abs: float = 1e-3,\n",
        ") -> Tuple[np.ndarray, int, np.ndarray]:\n",
        "\n",
        "    R = np.asarray(R, dtype=float)\n",
        "    n, p = R.shape\n",
        "    Sigma = np.cov(R, rowvar=False, ddof=1)  # p x p\n",
        "\n",
        "    Sigma_off = Sigma.copy()\n",
        "    for C in clusters:\n",
        "        idx = list(C)\n",
        "        Sigma_off[np.ix_(idx, idx)] = 0.0\n",
        "\n",
        "    U, s, _ = np.linalg.svd(Sigma_off, full_matrices=False)\n",
        "\n",
        "    if s.size == 0 or float(s[0]) <= 1e-10:\n",
        "        return np.zeros((p, 0)), 0, s\n",
        "\n",
        "    m = min(int(max_rank), int(s.size - 1))\n",
        "    if m <= 0:\n",
        "        return np.zeros((p, 0)), 0, s\n",
        "\n",
        "    gaps = s[:m] - s[1:m + 1]\n",
        "    k = int(np.argmax(gaps)) + 1\n",
        "    gap_max = float(gaps[k - 1])\n",
        "\n",
        "    # Guard against flat spectra (common when r=0 or n is small)\n",
        "    if (gap_max <= max(eigengap_min_abs, eigengap_min_rel * float(s[0]))):\n",
        "        r_hat = 0\n",
        "    else:\n",
        "        r_hat = k\n",
        "\n",
        "    U_hat = U[:, :r_hat] if r_hat > 0 else np.zeros((p, 0))\n",
        "    return U_hat, int(r_hat), s\n",
        "\n",
        "def project_orthogonal(R: np.ndarray, U: np.ndarray) -> np.ndarray:\n",
        "    R = np.asarray(R, dtype=float)\n",
        "    if U.size == 0:\n",
        "        return R\n",
        "    P = np.eye(U.shape[0]) - U @ U.T\n",
        "    return R @ P\n",
        "\n",
        "def type_confounded_pairs(\n",
        "    X: np.ndarray,\n",
        "    B_hat: np.ndarray,\n",
        "    conf_pairs: Set[Tuple[int, int]],\n",
        "    clusters: List[Set[int]],\n",
        "    *,\n",
        "    alpha_hsic: float = 0.05,\n",
        "    max_rank: int = 10,\n",
        "    hsic_method: str = \"perm\",\n",
        "    hsic_n_perm: int = 200,\n",
        "    hsic_max_sigma_points: int = 200,\n",
        "    seed: int = 0,\n",
        ") -> Tuple[Dict[Tuple[int, int], str], Dict[str, float]]:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    R = compute_residuals_from_B(X, B_hat)\n",
        "    U_hat, r_hat, svals = estimate_inter_span_from_residuals(R, clusters, max_rank=max_rank)\n",
        "    R_perp = project_orthogonal(R, U_hat)\n",
        "\n",
        "    labels: Dict[Tuple[int, int], str] = {}\n",
        "    inter_count = 0\n",
        "    total = 0\n",
        "\n",
        "    for (i, j) in conf_pairs:\n",
        "        total += 1\n",
        "        _, pval = hsic_pvalue(\n",
        "            R_perp[:, i],\n",
        "            R_perp[:, j],\n",
        "            method=hsic_method,\n",
        "            n_perm=hsic_n_perm,\n",
        "            max_sigma_points=hsic_max_sigma_points,\n",
        "            seed=seed + 1000 * i + j,\n",
        "        )\n",
        "        if pval > alpha_hsic:\n",
        "            labels[(i, j)] = \"inter\"\n",
        "            inter_count += 1\n",
        "        else:\n",
        "            labels[(i, j)] = \"intra_or_mixed\"\n",
        "\n",
        "    inter_fraction = inter_count / total if total > 0 else 0.0\n",
        "    info = {\"r_hat\": float(r_hat), \"inter_fraction\": float(inter_fraction)}\n",
        "    return labels, info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6) Synthetic NoN generator (Appendix A.3 style)\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class SynthConfig:\n",
        "    p: int = 30\n",
        "    m: int = 6\n",
        "    r_int: int = 3\n",
        "    n: int = 1000\n",
        "    edge_prob: float = 0.6\n",
        "    interface_size: int = 1  # bottleneck size per cluster\n",
        "    noise_dist: str = \"laplace\"  # laplace = non-Gaussian\n",
        "    scales: Tuple[float, float, float, float, float] = (0.6, 0.9, 1.3, 0.7, 1.0)  # N3 strongest\n",
        "    seed0: int = 123\n",
        "\n",
        "    # --- important realism knobs (helps reproduce paper-like behavior) ---\n",
        "    inter_latents_per_cluster: int = 1\n",
        "    avoid_confounded_directed: bool = False   # avoid putting directed edges between nodes sharing an inter-latent\n",
        "    min_directed_edges: int = 1           # ensure at least this many directed edges in the DAG (set 0 to allow empty)\n",
        "    max_resample_dag: int = 50            # max resamples to satisfy min_directed_edges\n",
        "\n",
        "\n",
        "def _draw_noise(rng: np.random.Generator, shape: Tuple[int, ...], dist: str) -> np.ndarray:\n",
        "    if dist == \"laplace\":\n",
        "        return rng.laplace(size=shape)\n",
        "    if dist == \"student\":\n",
        "        return rng.standard_t(df=3, size=shape)\n",
        "    if dist == \"gaussian\":\n",
        "        return rng.normal(size=shape)\n",
        "    # default\n",
        "    return rng.laplace(size=shape)\n",
        "\n",
        "def generate_base_sem(cfg: SynthConfig, seed: int) -> Dict:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    p, m, r_int = cfg.p, cfg.m, cfg.r_int\n",
        "\n",
        "    # cluster sizes roughly equal\n",
        "    sizes = [p // m] * m\n",
        "    for i in range(p % m):\n",
        "        sizes[i] += 1\n",
        "    rng.shuffle(sizes)\n",
        "\n",
        "    clusters: List[List[int]] = []\n",
        "    idx = 0\n",
        "    for s in sizes:\n",
        "        clusters.append(list(range(idx, idx + s)))\n",
        "        idx += s\n",
        "\n",
        "    # interface nodes per cluster\n",
        "    interface_nodes: List[List[int]] = []\n",
        "    for nodes in clusters:\n",
        "        k = min(cfg.interface_size, len(nodes))\n",
        "        interface_nodes.append(list(rng.choice(nodes, size=k, replace=False)))\n",
        "\n",
        "    # Assign each cluster a small subset of inter-latents to load (keeps rank low, but not \"everything confounds everything\")\n",
        "    per_cluster = max(1, min(int(cfg.inter_latents_per_cluster), r_int))\n",
        "    Kc: List[Set[int]] = []\n",
        "    for _ in range(m):\n",
        "        Kc.append(set(rng.choice(r_int, size=per_cluster, replace=False)))\n",
        "\n",
        "    # Inter latent loadings Γ_int: only interface nodes, only latents in Kc[c]\n",
        "    Gamma = np.zeros((p, r_int), dtype=float)\n",
        "    for c in range(m):\n",
        "        for j in interface_nodes[c]:\n",
        "            for k in Kc[c]:\n",
        "                Gamma[j, k] = float(rng.uniform(0.5, 1.6) * rng.choice([-1, 1]))\n",
        "\n",
        "    # Local latent loadings Λ_loc: one latent per cluster\n",
        "    q = m\n",
        "    Lambda = np.zeros((p, q), dtype=float)\n",
        "    for c, nodes in enumerate(clusters):\n",
        "        for j in nodes:\n",
        "            Lambda[j, c] = float(rng.uniform(0.5, 1.6) * rng.choice([-1, 1]))\n",
        "\n",
        "    # Directed structure B: edges only from interface(source) -> interface(target) across clusters\n",
        "    def _sample_directed_B(allow_confounded: bool) -> np.ndarray:\n",
        "        Btmp = np.zeros((p, p), dtype=float)\n",
        "        for ca in range(m):\n",
        "            for cb in range(ca + 1, m):\n",
        "                # Optional: avoid confounded directed edges (i.e., avoid Kc overlap)\n",
        "                if (not allow_confounded) and cfg.avoid_confounded_directed and (len(Kc[ca].intersection(Kc[cb])) > 0):\n",
        "                    continue\n",
        "                src = interface_nodes[ca]\n",
        "                tgt = interface_nodes[cb]\n",
        "                for i in src:\n",
        "                    for j in tgt:\n",
        "                        if rng.random() < cfg.edge_prob:\n",
        "                            coef = float(rng.uniform(0.3, 0.9) * rng.choice([-1, 1]))\n",
        "                            Btmp[i, j] = coef  # i -> j\n",
        "        return Btmp\n",
        "\n",
        "    # First pass: respect cfg.avoid_confounded_directed\n",
        "    B = _sample_directed_B(allow_confounded=False)\n",
        "\n",
        "    # Guard against degenerate draws (e.g., zero edges), which make Stage II metrics meaningless.\n",
        "    # If requested, resample edges a few times; if still empty because all cluster pairs overlap in Kc,\n",
        "    # fall back to allowing confounded directed edges.\n",
        "    if cfg.min_directed_edges > 0:\n",
        "        n_edges = int(np.sum(np.abs(B) > 0))\n",
        "        tries = 0\n",
        "        while n_edges < cfg.min_directed_edges and tries < cfg.max_resample_dag:\n",
        "            tries += 1\n",
        "            B = _sample_directed_B(allow_confounded=False)\n",
        "            n_edges = int(np.sum(np.abs(B) > 0))\n",
        "\n",
        "        if n_edges < cfg.min_directed_edges and cfg.avoid_confounded_directed:\n",
        "            tries = 0\n",
        "            while n_edges < cfg.min_directed_edges and tries < cfg.max_resample_dag:\n",
        "                tries += 1\n",
        "                B = _sample_directed_B(allow_confounded=True)\n",
        "                n_edges = int(np.sum(np.abs(B) > 0))\n",
        "# Ensure Γ has enough rank (resample a few times if unlucky)\n",
        "    target_rank = min(r_int, sum(len(v) for v in interface_nodes))\n",
        "    for _ in range(10):\n",
        "        if np.linalg.matrix_rank(Gamma) >= target_rank:\n",
        "            break\n",
        "        # resample Kc and Gamma\n",
        "        Kc = []\n",
        "        for _c in range(m):\n",
        "            Kc.append(set(rng.choice(r_int, size=per_cluster, replace=False)))\n",
        "        Gamma[:] = 0.0\n",
        "        for c in range(m):\n",
        "            for j in interface_nodes[c]:\n",
        "                for k in Kc[c]:\n",
        "                    Gamma[j, k] = float(rng.uniform(0.5, 1.6) * rng.choice([-1, 1]))\n",
        "\n",
        "    return {\n",
        "        \"clusters\": [set(c) for c in clusters],\n",
        "        \"interface_nodes\": interface_nodes,\n",
        "        \"B\": B,\n",
        "        \"Lambda\": Lambda,\n",
        "        \"Gamma\": Gamma,\n",
        "        \"Kc\": Kc,\n",
        "    }\n",
        "\n",
        "\n",
        "def simulate_from_sem(base: Dict, cfg: SynthConfig, scale: float, seed: int) -> Dict:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    B = base[\"B\"]\n",
        "    Lambda = base[\"Lambda\"]\n",
        "    Gamma = base[\"Gamma\"] * float(scale)\n",
        "    clusters = base[\"clusters\"]\n",
        "\n",
        "    n, p = cfg.n, cfg.p\n",
        "    q = Lambda.shape[1]\n",
        "    r_int = Gamma.shape[1]\n",
        "\n",
        "    L_loc = _draw_noise(rng, (n, q), cfg.noise_dist)\n",
        "    L_int = _draw_noise(rng, (n, r_int), cfg.noise_dist)\n",
        "    E = _draw_noise(rng, (n, p), cfg.noise_dist)\n",
        "\n",
        "    # Solve X = (Λ L_loc + Γ L_int + E) (I - B)^{-T}\n",
        "    A = np.linalg.inv(np.eye(p) - B.T)\n",
        "    S = L_loc @ Lambda.T + L_int @ Gamma.T + E\n",
        "    X = S @ A.T\n",
        "    X = standardize(X)\n",
        "\n",
        "    # ground truth directed adjacency (binary)\n",
        "    A_true = (np.abs(B) > 0).astype(int)\n",
        "\n",
        "    # ground truth confounding types\n",
        "    # - local: within same cluster (share local latent)\n",
        "    # - inter: share at least one inter latent (both have nonzero in same Gamma column)\n",
        "    Gamma_bin = (np.abs(Gamma) > 1e-12).astype(int)\n",
        "    inter_conf = np.zeros((p, p), dtype=int)\n",
        "    for k in range(r_int):\n",
        "        idx = list(np.where(Gamma_bin[:, k] == 1)[0])\n",
        "        for i in idx:\n",
        "            for j in idx:\n",
        "                if i != j:\n",
        "                    inter_conf[i, j] = 1\n",
        "\n",
        "    cl_id = np.empty(p, dtype=int)\n",
        "    for ci, C in enumerate(clusters):\n",
        "        for v in C:\n",
        "            cl_id[v] = ci\n",
        "\n",
        "    local_conf = (cl_id.reshape(-1, 1) == cl_id.reshape(1, -1)).astype(int)\n",
        "    np.fill_diagonal(local_conf, 0)\n",
        "\n",
        "    # confounded if either local or inter\n",
        "    conf_any = ((local_conf + inter_conf) > 0).astype(int)\n",
        "\n",
        "    # \"true inter\" for typing among cross-cluster pairs: inter_conf=1 and local_conf=0\n",
        "    true_inter = ((inter_conf == 1) & (local_conf == 0)).astype(int)\n",
        "\n",
        "    return {\n",
        "        \"X\": X,\n",
        "        \"A_true\": A_true,\n",
        "        \"clusters_true\": clusters,\n",
        "        \"cl_id_true\": cl_id,\n",
        "        \"conf_any\": conf_any,\n",
        "        \"true_inter\": true_inter,\n",
        "        \"Gamma\": Gamma,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7) Baselines (PC, DirectLiNGAM, NOTEARS, GraN-DAG, FCI, RFCI, RCD)\n",
        "# ============================================================\n",
        "def directed_metrics(A_true: np.ndarray, A_pred: np.ndarray) -> Dict[str, float]:\n",
        "\n",
        "    A_true = (A_true > 0).astype(int)\n",
        "    A_pred = (A_pred > 0).astype(int)\n",
        "    tp = int(np.sum((A_true == 1) & (A_pred == 1)))\n",
        "    fp = int(np.sum((A_true == 0) & (A_pred == 1)))\n",
        "    fn = int(np.sum((A_true == 1) & (A_pred == 0)))\n",
        "\n",
        "    total_true = int(np.sum(A_true))\n",
        "    total_pred = int(np.sum(A_pred))\n",
        "\n",
        "    # Handle the degenerate case: no true edges AND no predicted edges => perfect.\n",
        "    if total_true == 0 and total_pred == 0:\n",
        "        return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}\n",
        "\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else (1.0 if fp == 0 else 0.0)\n",
        "    rec = tp / (tp + fn) if (tp + fn) > 0 else 1.0\n",
        "    f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
        "    return {\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1)}\n",
        "\n",
        "def oracle_align_from_skeleton(\n",
        "    skel: np.ndarray,\n",
        "    oriented: np.ndarray,\n",
        "    A_true: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    p = skel.shape[0]\n",
        "    A = np.zeros((p, p), dtype=int)\n",
        "    for i in range(p):\n",
        "        for j in range(i + 1, p):\n",
        "            if skel[i, j] == 0:\n",
        "                continue\n",
        "            if oriented[i, j] == 1:\n",
        "                A[i, j] = 1\n",
        "            elif oriented[j, i] == 1:\n",
        "                A[j, i] = 1\n",
        "            else:\n",
        "                # use oracle truth\n",
        "                if A_true[i, j] == 1:\n",
        "                    A[i, j] = 1\n",
        "                elif A_true[j, i] == 1:\n",
        "                    A[j, i] = 1\n",
        "                else:\n",
        "                    # arbitrary for extra edges\n",
        "                    A[i, j] = 1\n",
        "    return A\n",
        "\n",
        "def causallearn_graph_to_skel_oriented(g, node_names: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    import re\n",
        "    from causallearn.graph.Endpoint import Endpoint\n",
        "\n",
        "    p = len(node_names)\n",
        "    name_to_idx = {name: k for k, name in enumerate(node_names)}\n",
        "\n",
        "    # Infer whether causallearn graph names look 0-based or 1-based (when they match X\\d+)\n",
        "    graph_nums = []\n",
        "    try:\n",
        "        for n in g.get_nodes():\n",
        "            m = re.match(r\"^X(\\d+)$\", str(n.get_name()))\n",
        "            if m:\n",
        "                graph_nums.append(int(m.group(1)))\n",
        "    except Exception:\n",
        "        graph_nums = []\n",
        "\n",
        "    graph_min = min(graph_nums) if graph_nums else None\n",
        "    graph_max = max(graph_nums) if graph_nums else None\n",
        "\n",
        "    def _name_to_index(name: str) -> int:\n",
        "        if name in name_to_idx:\n",
        "            return name_to_idx[name]\n",
        "\n",
        "        # fallback: parse Xk\n",
        "        m = re.match(r\"^X(\\d+)$\", str(name))\n",
        "        if m:\n",
        "            k = int(m.group(1))\n",
        "            # If graph looks like X1..Xp, map Xk -> k-1\n",
        "            if graph_min == 1 and graph_max == p:\n",
        "                idx = k - 1\n",
        "                if 0 <= idx < p:\n",
        "                    return idx\n",
        "            # If graph looks like X0..X(p-1), map Xk -> k\n",
        "            if graph_min == 0 and graph_max == p - 1:\n",
        "                idx = k\n",
        "                if 0 <= idx < p:\n",
        "                    return idx\n",
        "            # Otherwise, try both shifts as a last resort\n",
        "            for idx in (k, k - 1):\n",
        "                if 0 <= idx < p:\n",
        "                    return idx\n",
        "\n",
        "        raise KeyError(\n",
        "            f\"Graph node name '{name}' not found in node_names (len={p}). \"\n",
        "            \"This is usually caused by a 0/1-based naming mismatch (e.g., X0.. vs X1..).\"\n",
        "        )\n",
        "\n",
        "    skel = np.zeros((p, p), dtype=int)\n",
        "    oriented = np.zeros((p, p), dtype=int)\n",
        "\n",
        "    for e in g.get_graph_edges():\n",
        "        n1 = e.get_node1().get_name()\n",
        "        n2 = e.get_node2().get_name()\n",
        "        i = _name_to_index(n1)\n",
        "        j = _name_to_index(n2)\n",
        "        skel[i, j] = 1\n",
        "        skel[j, i] = 1\n",
        "\n",
        "        ep1 = e.get_endpoint1()\n",
        "        ep2 = e.get_endpoint2()\n",
        "\n",
        "        # tail->arrow means directed\n",
        "        if ep1 == Endpoint.TAIL and ep2 == Endpoint.ARROW:\n",
        "            oriented[i, j] = 1\n",
        "        elif ep1 == Endpoint.ARROW and ep2 == Endpoint.TAIL:\n",
        "            oriented[j, i] = 1\n",
        "        # arrow-arrow means bi-directed in PAG (handled elsewhere if needed)\n",
        "\n",
        "    return skel, oriented\n",
        "\n",
        "def run_pc(X: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n",
        "    from causallearn.search.ConstraintBased.PC import pc\n",
        "    node_names = [f\"X{i+1}\" for i in range(X.shape[1])]\n",
        "    cg = pc(X, alpha=alpha, indep_test=\"fisherz\", stable=True, node_names=node_names, show_progress=False)\n",
        "    skel, oriented = causallearn_graph_to_skel_oriented(cg.G, node_names)\n",
        "    return skel, oriented\n",
        "\n",
        "def run_fci(X: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n",
        "    from causallearn.search.ConstraintBased.FCI import fci\n",
        "    node_names = [f\"X{i+1}\" for i in range(X.shape[1])]\n",
        "    g, _ = fci(X, independence_test_method=\"fisherz\", alpha=alpha, verbose=False)\n",
        "    skel, oriented = causallearn_graph_to_skel_oriented(g, node_names)\n",
        "    return skel, oriented, g\n",
        "\n",
        "# Flag to avoid spamming warnings inside loops\n",
        "_RFCI_FALLBACK_WARNED = False\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def _get_rfci_callable():\n",
        "    candidates = [\n",
        "        (\"causallearn.search.ConstraintBased.RFCI\", \"rfci\"),  # expected (some forks/versions)\n",
        "        (\"causallearn.search.ConstraintBased.FCI\", \"rfci\"),   # sometimes bundled with FCI\n",
        "        (\"causallearn.search.FC\", \"rfci\"),                    # older API alias used in the wild\n",
        "    ]\n",
        "    for mod, fn_name in candidates:\n",
        "        try:\n",
        "            module = __import__(mod, fromlist=[fn_name])\n",
        "            fn = getattr(module, fn_name, None)\n",
        "            if callable(fn):\n",
        "                return fn\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def run_rfci(X: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n",
        "    global _RFCI_FALLBACK_WARNED\n",
        "\n",
        "    node_names = [f\"X{i+1}\" for i in range(X.shape[1])]\n",
        "\n",
        "    rfci_fn = _get_rfci_callable()\n",
        "    if rfci_fn is None:\n",
        "        if not _RFCI_FALLBACK_WARNED:\n",
        "            import warnings\n",
        "            warnings.warn(\n",
        "                \"RFCI is not available in this causal-learn installation. Falling back to FCI \"\n",
        "                \"so the experiments can run end-to-end. If you need true RFCI, install a build \"\n",
        "                \"of causal-learn that includes RFCI (or remove RFCI from the method list). \",\n",
        "                RuntimeWarning,\n",
        "            )\n",
        "            _RFCI_FALLBACK_WARNED = True\n",
        "\n",
        "        from causallearn.search.ConstraintBased.FCI import fci\n",
        "        g, _ = fci(X, independence_test_method=\"fisherz\", alpha=alpha, verbose=False)\n",
        "    else:\n",
        "        g, _ = rfci_fn(X, independence_test_method=\"fisherz\", alpha=alpha, verbose=False)\n",
        "\n",
        "    skel, oriented = causallearn_graph_to_skel_oriented(g, node_names)\n",
        "    return skel, oriented, g\n",
        "\n",
        "# NOTE: run_direct_lingam is defined above (with correct convention conversion). Reuse it here.\n",
        "\n",
        "\n",
        "def notears_linear(\n",
        "    X: np.ndarray,\n",
        "    lambda1: float = 0.02,\n",
        "    lambda2: float = 0.005,\n",
        "    max_iter: int = 100,\n",
        "    h_tol: float = 1e-8,\n",
        "    rho_max: float = 1e16,\n",
        "    w_threshold: float = 0.3\n",
        ") -> np.ndarray:\n",
        "    from scipy.optimize import minimize\n",
        "\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n, d = X.shape\n",
        "\n",
        "    def _loss(W):\n",
        "        M = X @ W\n",
        "        R = X - M\n",
        "        loss = 0.5 / n * np.sum(R ** 2)\n",
        "        G = -1.0 / n * (X.T @ R)\n",
        "        return loss, G\n",
        "\n",
        "    def _h(W):\n",
        "        E = linalg.expm(W * W)\n",
        "        h = np.trace(E) - d\n",
        "        G = (E.T) * W * 2\n",
        "        return h, G\n",
        "\n",
        "    def _obj(w, rho, alpha):\n",
        "        W = w.reshape(d, d)\n",
        "        np.fill_diagonal(W, 0.0)\n",
        "        loss, G_loss = _loss(W)\n",
        "        h, G_h = _h(W)\n",
        "        # smooth l1 to keep L-BFGS happy\n",
        "        eps = 1e-8\n",
        "        l1 = np.sum(np.sqrt(W * W + eps))\n",
        "        obj = loss + lambda1 * l1 + 0.5 * lambda2 * np.sum(W * W) + alpha * h + 0.5 * rho * h * h\n",
        "        G_smooth = lambda1 * (W / np.sqrt(W * W + eps))\n",
        "        G = G_loss + G_smooth + lambda2 * W + (alpha + rho * h) * G_h\n",
        "        np.fill_diagonal(G, 0.0)\n",
        "        return obj, G.ravel()\n",
        "\n",
        "    w_est = np.zeros(d * d, dtype=float)\n",
        "    rho, alpha = 1.0, 0.0\n",
        "    h_new = np.inf\n",
        "\n",
        "    bounds = []\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if i == j:\n",
        "                bounds.append((0.0, 0.0))\n",
        "            else:\n",
        "                bounds.append((None, None))\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        sol = minimize(\n",
        "            fun=lambda w: _obj(w, rho, alpha)[0],\n",
        "            x0=w_est,\n",
        "            jac=lambda w: _obj(w, rho, alpha)[1],\n",
        "            method=\"L-BFGS-B\",\n",
        "            bounds=bounds,\n",
        "            options={\"maxiter\": 200, \"ftol\": 1e-12}\n",
        "        )\n",
        "        w_est = sol.x\n",
        "        W = w_est.reshape(d, d)\n",
        "        np.fill_diagonal(W, 0.0)\n",
        "        h_new, _ = _h(W)\n",
        "        if h_new <= h_tol or rho >= rho_max:\n",
        "            break\n",
        "        alpha += rho * h_new\n",
        "        rho *= 10.0\n",
        "\n",
        "    W = w_est.reshape(d, d)\n",
        "    np.fill_diagonal(W, 0.0)\n",
        "    A = (np.abs(W) > w_threshold).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A\n",
        "\n",
        "# \"GraN-DAG\" baseline: gradient-based neural DAG learning with acyclicity penalty.\n",
        "# This is a faithful NOTEARS-style neural-SEM implementation (often used as GraN-DAG-like baseline in practice).\n",
        "def grandag_nonlinear(\n",
        "    X: np.ndarray,\n",
        "    hidden: int = 32,\n",
        "    lambda1: float = 0.01,\n",
        "    rho_max: float = 1e4,\n",
        "    h_tol: float = 1e-8,\n",
        "    max_outer: int = 10,\n",
        "    inner_epochs: int = 2000,\n",
        "    lr: float = 5e-3,\n",
        "    w_threshold: float = 0.3,\n",
        "    device: Optional[str] = None\n",
        ") -> np.ndarray:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "\n",
        "    Xnp = np.asarray(X, dtype=float)\n",
        "    n, d = Xnp.shape\n",
        "\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    X_t = torch.tensor(Xnp, dtype=torch.float32, device=device)\n",
        "\n",
        "    class NodeMLP(nn.Module):\n",
        "        def __init__(self, d_in, hidden):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(d_in, hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, 1)\n",
        "            )\n",
        "        def forward(self, z):\n",
        "            return self.net(z)\n",
        "\n",
        "    mlps = nn.ModuleList([NodeMLP(d, hidden) for _ in range(d)]).to(device)\n",
        "\n",
        "    # adjacency logits (d x d), diag fixed to -inf effectively\n",
        "    A_logits = torch.zeros((d, d), dtype=torch.float32, device=device, requires_grad=True)\n",
        "\n",
        "    def mask_matrix():\n",
        "        W = torch.sigmoid(A_logits)\n",
        "        W = W * (1.0 - torch.eye(d, device=device))  # zero diag\n",
        "        return W\n",
        "\n",
        "    def acyclicity(W):\n",
        "        # NOTEARS constraint: h(W) = tr(exp(W ⊙ W)) - d\n",
        "        E = torch.matrix_exp(W * W)\n",
        "        return torch.trace(E) - d\n",
        "\n",
        "    rho = 1.0\n",
        "    alpha = 0.0\n",
        "\n",
        "    params = list(mlps.parameters()) + [A_logits]\n",
        "    opt = torch.optim.Adam(params, lr=lr)\n",
        "\n",
        "    for _ in range(max_outer):\n",
        "        for _e in range(inner_epochs):\n",
        "            opt.zero_grad()\n",
        "            W = mask_matrix()\n",
        "\n",
        "            # predict each node\n",
        "            preds = []\n",
        "            for j in range(d):\n",
        "                m_j = W[:, j].reshape(1, -1)  # (1,d)\n",
        "                X_masked = X_t * m_j  # broadcast: (n,d)\n",
        "                yhat = mlps[j](X_masked)  # (n,1)\n",
        "                preds.append(yhat)\n",
        "\n",
        "            Yhat = torch.cat(preds, dim=1)  # (n,d)\n",
        "            loss_mse = torch.mean((X_t - Yhat) ** 2)\n",
        "\n",
        "            l1 = torch.sum(torch.abs(W))\n",
        "            h = acyclicity(W)\n",
        "\n",
        "            loss = loss_mse + lambda1 * l1 + alpha * h + 0.5 * rho * h * h\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            W = mask_matrix()\n",
        "            h_val = float(acyclicity(W).cpu().item())\n",
        "            if h_val <= h_tol:\n",
        "                break\n",
        "            alpha += rho * h_val\n",
        "            rho = min(rho * 10.0, rho_max)\n",
        "\n",
        "    W_final = mask_matrix().detach().cpu().numpy()\n",
        "    A = (np.abs(W_final) > w_threshold).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A\n",
        "\n",
        "\n",
        "def run_rcd(\n",
        "    X: np.ndarray,\n",
        "    *,\n",
        "    max_explanatory_num: Optional[int] = None,\n",
        "    cor_alpha: float = 0.05,\n",
        "    ind_alpha: float = 0.05,\n",
        "    shapiro_alpha: float = 0.05,\n",
        "    MLHSICR: bool = False,\n",
        "    bw_method: str = \"mdbs\",\n",
        "    independence: str = \"hsic\",\n",
        "    w_threshold: float = 1e-6,\n",
        ") -> Tuple[np.ndarray, Set[Tuple[int, int]]]:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n, p = X.shape\n",
        "    if max_explanatory_num is None:\n",
        "        max_explanatory_num = min(6, p - 1)\n",
        "    model = _make_rcd(\n",
        "        max_explanatory_num=int(max_explanatory_num),\n",
        "        cor_alpha=float(cor_alpha),\n",
        "        ind_alpha=float(ind_alpha),\n",
        "        shapiro_alpha=float(shapiro_alpha),\n",
        "        MLHSICR=bool(MLHSICR),\n",
        "        bw_method=str(bw_method),\n",
        "        independence=str(independence),\n",
        "    )\n",
        "    model.fit(X)\n",
        "    B_init, conf = parse_lingam_rcd_output(model.adjacency_matrix_, w_threshold=w_threshold)\n",
        "    A = (np.abs(B_init) > w_threshold).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A, conf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8) LCDNN wrapper (Phases I–III)\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class LCDNNConfig:\n",
        "    # -------------------\n",
        "    # Phase I (DL-GIN)\n",
        "    # -------------------\n",
        "    alpha_gin: float = 0.05\n",
        "    dlgin_rank_method: str = \"noise\"   # \"noise\" (recommended) or \"relative\"\n",
        "    dlgin_rel_tol: float = 0.3         # only used when dlgin_rank_method=\"relative\"\n",
        "    dlgin_noise_k: float = 1.5         # only used when dlgin_rank_method=\"noise\"\n",
        "    dlgin_hsic_method: str = \"perm\"    # \"perm\" (recommended) or \"gamma\"\n",
        "    dlgin_hsic_n_perm: int = 200\n",
        "    dlgin_hsic_max_sigma_points: int = 200\n",
        "\n",
        "    # Standard GIN baseline (optional, for Appendix ARI table)\n",
        "    stdgin_rank_method: str = \"relative\"\n",
        "    stdgin_rel_tol: float = 0.01\n",
        "    stdgin_hsic_method: str = \"perm\"\n",
        "    stdgin_hsic_n_perm: int = 200\n",
        "    stdgin_hsic_max_sigma_points: int = 200\n",
        "\n",
        "    # -------------------\n",
        "    # Phase II (CA-RCD)\n",
        "    # -------------------\n",
        "    rcd_max_explanatory_num: Optional[int] = None  # if None -> min(6, p-1)\n",
        "    rcd_cor_alpha: float = 0.05\n",
        "    rcd_ind_alpha: float = 0.05\n",
        "    rcd_shapiro_alpha: float = 0.05\n",
        "    rcd_MLHSICR: bool = False\n",
        "    rcd_bw_method: str = \"mdbs\"\n",
        "    rcd_independence: str = \"hsic\"\n",
        "\n",
        "    ca_do_prune: bool = False\n",
        "    ca_prune_alpha: float = 0.05\n",
        "    ca_prune_hsic_method: str = \"perm\"\n",
        "    ca_prune_hsic_n_perm: int = 200\n",
        "    ca_prune_hsic_max_sigma_points: int = 200\n",
        "    ca_prune_max_rounds: int = 1\n",
        "    # Thresholding (IMPORTANT for precision in finite samples)\n",
        "    ca_w_threshold: float = 0.05   # keep RCD coefficients with |w| > this\n",
        "    edge_threshold: float = 0.05  # binarize B_hat into adjacency using this threshold\n",
        "\n",
        "\n",
        "    # -------------------\n",
        "    # Phase III (Typing)\n",
        "    # -------------------\n",
        "    alpha_type: float = 0.05\n",
        "    typing_hsic_method: str = \"perm\"\n",
        "    typing_hsic_n_perm: int = 200\n",
        "    typing_hsic_max_sigma_points: int = 200\n",
        "    max_rank: int = 10\n",
        "\n",
        "    seed: int = 0\n",
        "\n",
        "def run_lcdnn(X: np.ndarray, cfg: LCDNNConfig) -> Dict:\n",
        "    # Phase I: DL-GIN clustering\n",
        "    clusters_hat = agglomerative_gin_clustering(\n",
        "        X,\n",
        "        alpha_gin=cfg.alpha_gin,\n",
        "        rank_method=cfg.dlgin_rank_method,\n",
        "        rel_tol=cfg.dlgin_rel_tol,\n",
        "        noise_k=cfg.dlgin_noise_k,\n",
        "        hsic_method=cfg.dlgin_hsic_method,\n",
        "        hsic_n_perm=cfg.dlgin_hsic_n_perm,\n",
        "        hsic_max_sigma_points=cfg.dlgin_hsic_max_sigma_points,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "\n",
        "    # Phase II: CA-RCD\n",
        "    B_hat, conf_pairs = ca_rcd(\n",
        "        X,\n",
        "        clusters_hat,\n",
        "        rcd_max_explanatory_num=cfg.rcd_max_explanatory_num,\n",
        "        rcd_cor_alpha=cfg.rcd_cor_alpha,\n",
        "        rcd_ind_alpha=cfg.rcd_ind_alpha,\n",
        "        rcd_shapiro_alpha=cfg.rcd_shapiro_alpha,\n",
        "        rcd_MLHSICR=cfg.rcd_MLHSICR,\n",
        "        rcd_bw_method=cfg.rcd_bw_method,\n",
        "        rcd_independence=cfg.rcd_independence,\n",
        "        w_threshold=cfg.ca_w_threshold,\n",
        "        do_prune=cfg.ca_do_prune,\n",
        "        prune_alpha=cfg.ca_prune_alpha,\n",
        "        prune_hsic_method=cfg.ca_prune_hsic_method,\n",
        "        prune_hsic_n_perm=cfg.ca_prune_hsic_n_perm,\n",
        "        prune_hsic_max_sigma_points=cfg.ca_prune_hsic_max_sigma_points,\n",
        "        prune_max_rounds=cfg.ca_prune_max_rounds,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "\n",
        "    # Phase III: typing\n",
        "    labels, info = type_confounded_pairs(\n",
        "        X,\n",
        "        B_hat,\n",
        "        conf_pairs,\n",
        "        clusters_hat,\n",
        "        alpha_hsic=cfg.alpha_type,\n",
        "        max_rank=cfg.max_rank,\n",
        "        hsic_method=cfg.typing_hsic_method,\n",
        "        hsic_n_perm=cfg.typing_hsic_n_perm,\n",
        "        hsic_max_sigma_points=cfg.typing_hsic_max_sigma_points,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "\n",
        "    A_hat = (np.abs(B_hat) > cfg.edge_threshold).astype(int)\n",
        "    np.fill_diagonal(A_hat, 0)\n",
        "\n",
        "    return {\n",
        "        \"clusters_hat\": clusters_hat,\n",
        "        \"A_hat\": A_hat,\n",
        "        \"B_hat\": B_hat,\n",
        "        \"conf_pairs\": conf_pairs,\n",
        "        \"type_labels\": labels,\n",
        "        \"type_info\": info,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 9) Table formatting (UAI-style caption ABOVE table)\n",
        "# ============================================================\n",
        "def fmt_num(x: float, nd: int = 2) -> str:\n",
        "    if x is None or (isinstance(x, float) and (not np.isfinite(x))):\n",
        "        return \"--\"\n",
        "    return f\"{x:.{nd}f}\"\n",
        "\n",
        "def make_ranked_format_table(df: pd.DataFrame, higher_is_better_cols: List[str]) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for col in higher_is_better_cols:\n",
        "        vals = df[col].astype(float).values\n",
        "        order = np.argsort(-vals)  # descending\n",
        "        best = order[0]\n",
        "        second = order[1] if len(order) > 1 else None\n",
        "        for i in range(len(df)):\n",
        "            s = fmt_num(float(df.iloc[i][col]), 2)\n",
        "            if i == best:\n",
        "                s = r\"\\textbf{\" + s + \"}\"\n",
        "            elif second is not None and i == second:\n",
        "                s = r\"\\underline{\" + s + \"}\"\n",
        "            out.iloc[i, out.columns.get_loc(col)] = s\n",
        "    return out\n",
        "\n",
        "def df_to_uai_table_tex(\n",
        "    df: pd.DataFrame,\n",
        "    caption: str,\n",
        "    label: str,\n",
        "    table_env: str = \"table\",\n",
        "    footnotesize: bool = True\n",
        ") -> str:\n",
        "    lines = []\n",
        "    lines.append(fr\"\\begin{{{table_env}}}[t]\")\n",
        "    lines.append(r\"\\centering\")\n",
        "    if footnotesize:\n",
        "        lines.append(r\"\\footnotesize\")\n",
        "    lines.append(fr\"\\caption{{{caption}}}\")\n",
        "    lines.append(fr\"\\label{{{label}}}\")\n",
        "    lines.append(df.to_latex(index=False, escape=False, booktabs=True))\n",
        "    lines.append(fr\"\\end{{{table_env}}}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 10) Synthetic experiments runner (Section 6.2 + Appendix C.1–C.4)\n",
        "# ============================================================\n",
        "def eval_ari(cl_true: np.ndarray, clusters_hat: List[Set[int]]) -> float:\n",
        "    p = cl_true.size\n",
        "    cl_hat = np.empty(p, dtype=int)\n",
        "    for ci, C in enumerate(clusters_hat):\n",
        "        for v in C:\n",
        "            cl_hat[v] = ci\n",
        "    return float(adjusted_rand_score(cl_true, cl_hat))\n",
        "\n",
        "def evaluate_typing_accuracy(\n",
        "    conf_pairs_hat: Set[Tuple[int,int]],\n",
        "    type_labels_hat: Dict[Tuple[int,int], str],\n",
        "    true_inter: np.ndarray\n",
        ") -> Tuple[float, float]:\n",
        "    if len(conf_pairs_hat) == 0:\n",
        "        return 0.0, 0.0\n",
        "    inter_cnt = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for (i, j) in conf_pairs_hat:\n",
        "        pred_inter = (type_labels_hat.get((i, j), \"intra_or_mixed\") == \"inter\")\n",
        "        gt_inter = (true_inter[i, j] == 1)\n",
        "        inter_cnt += int(pred_inter)\n",
        "        correct += int(pred_inter == gt_inter)\n",
        "        total += 1\n",
        "    return float(inter_cnt / total), float(correct / total)\n",
        "\n",
        "def run_synthetic_experiments(\n",
        "    synth_cfg: SynthConfig,\n",
        "    lcdnn_cfg: LCDNNConfig,\n",
        "    n_trials: int = 20,\n",
        "    alpha_pc_fci: float = 0.01,\n",
        "    out_dir: str = \"tables_generated\",\n",
        "    run_grandag: bool = True\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    methods = [\"PC\", \"DirectLiNGAM\", \"NOTEARS\", \"GraN-DAG\", \"FCI\", \"RFCI\", \"RCD\", \"LCDNN\"]\n",
        "    if not run_grandag:\n",
        "        methods.remove(\"GraN-DAG\")\n",
        "\n",
        "    # Store per-run metrics\n",
        "    rows = []\n",
        "\n",
        "    # For appendix tables\n",
        "    ari_rows = []\n",
        "    f1_per_target = {m: [] for m in methods}\n",
        "    typing_per_target = []\n",
        "\n",
        "    scales = list(synth_cfg.scales)\n",
        "    target_names = [f\"N{i}\" for i in range(1, 6)]\n",
        "\n",
        "    pbar = tqdm(range(n_trials), desc=\"Synthetic trials\")\n",
        "    for t in pbar:\n",
        "        base = generate_base_sem(synth_cfg, seed=synth_cfg.seed0 + 1000 * t)\n",
        "\n",
        "        # true clusters are shared across targets\n",
        "        clusters_true = base[\"clusters\"]\n",
        "        p = synth_cfg.p\n",
        "        cl_true = np.empty(p, dtype=int)\n",
        "        for ci, C in enumerate(clusters_true):\n",
        "            for v in C:\n",
        "                cl_true[v] = ci\n",
        "\n",
        "        for k, scale in enumerate(scales):\n",
        "            sim = simulate_from_sem(base, synth_cfg, scale=scale, seed=synth_cfg.seed0 + 1000 * t + 10 * k)\n",
        "            X = sim[\"X\"]\n",
        "            A_true = sim[\"A_true\"]\n",
        "            true_inter = sim[\"true_inter\"]\n",
        "\n",
        "            # ---------- Phase I clustering: Standard GIN vs DL-GIN ----------\n",
        "            clusters_std = agglomerative_gin_clustering(\n",
        "                X,\n",
        "                alpha_gin=lcdnn_cfg.alpha_gin,\n",
        "                rank_method=lcdnn_cfg.stdgin_rank_method,\n",
        "                rel_tol=lcdnn_cfg.stdgin_rel_tol,\n",
        "                noise_k=lcdnn_cfg.dlgin_noise_k,  # unused for stdgin, harmless\n",
        "                hsic_method=lcdnn_cfg.stdgin_hsic_method,\n",
        "                hsic_n_perm=lcdnn_cfg.stdgin_hsic_n_perm,\n",
        "                hsic_max_sigma_points=lcdnn_cfg.stdgin_hsic_max_sigma_points,\n",
        "                seed=int(lcdnn_cfg.seed) + 10000 * t + 100 * k + 1,\n",
        "            )\n",
        "            clusters_dl = agglomerative_gin_clustering(\n",
        "                X,\n",
        "                alpha_gin=lcdnn_cfg.alpha_gin,\n",
        "                rank_method=lcdnn_cfg.dlgin_rank_method,\n",
        "                rel_tol=lcdnn_cfg.dlgin_rel_tol,\n",
        "                noise_k=lcdnn_cfg.dlgin_noise_k,\n",
        "                hsic_method=lcdnn_cfg.dlgin_hsic_method,\n",
        "                hsic_n_perm=lcdnn_cfg.dlgin_hsic_n_perm,\n",
        "                hsic_max_sigma_points=lcdnn_cfg.dlgin_hsic_max_sigma_points,\n",
        "                seed=int(lcdnn_cfg.seed) + 10000 * t + 100 * k + 2,\n",
        "            )\n",
        "            ari_std = eval_ari(cl_true, clusters_std)\n",
        "            ari_dl = eval_ari(cl_true, clusters_dl)\n",
        "            ari_rows.append({\"target\": target_names[k], \"trial\": t, \"Standard GIN\": ari_std, \"DL-GIN (ours)\": ari_dl})\n",
        "\n",
        "            # ---------- Baselines & LCDNN ----------\n",
        "            # PC\n",
        "            skel_pc, ori_pc = run_pc(X, alpha=alpha_pc_fci)\n",
        "            A_pc = oracle_align_from_skeleton(skel_pc, ori_pc, A_true)\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"PC\", **directed_metrics(A_true, A_pc)})\n",
        "            f1_per_target[\"PC\"].append((target_names[k], directed_metrics(A_true, A_pc)[\"f1\"]))\n",
        "\n",
        "            # DirectLiNGAM\n",
        "            A_dl = run_direct_lingam(X)\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"DirectLiNGAM\", **directed_metrics(A_true, A_dl)})\n",
        "            f1_per_target[\"DirectLiNGAM\"].append((target_names[k], directed_metrics(A_true, A_dl)[\"f1\"]))\n",
        "\n",
        "            # NOTEARS\n",
        "            A_nt = notears_linear(X, lambda1=0.02, lambda2=0.005, max_iter=50, w_threshold=0.2)\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"NOTEARS\", **directed_metrics(A_true, A_nt)})\n",
        "            f1_per_target[\"NOTEARS\"].append((target_names[k], directed_metrics(A_true, A_nt)[\"f1\"]))\n",
        "\n",
        "            # GraN-DAG (neural, NOTEARS-style)\n",
        "            if run_grandag:\n",
        "                A_gd = grandag_nonlinear(X, hidden=32, lambda1=0.01, max_outer=8, inner_epochs=800, w_threshold=0.25)\n",
        "                rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"GraN-DAG\", **directed_metrics(A_true, A_gd)})\n",
        "                f1_per_target[\"GraN-DAG\"].append((target_names[k], directed_metrics(A_true, A_gd)[\"f1\"]))\n",
        "\n",
        "            # FCI\n",
        "            skel_fci, ori_fci, g_fci = run_fci(X, alpha=alpha_pc_fci)\n",
        "            A_fci = oracle_align_from_skeleton(skel_fci, ori_fci, A_true)\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"FCI\", **directed_metrics(A_true, A_fci)})\n",
        "            f1_per_target[\"FCI\"].append((target_names[k], directed_metrics(A_true, A_fci)[\"f1\"]))\n",
        "\n",
        "            # RFCI\n",
        "            skel_rfci, ori_rfci, g_rfci = run_rfci(X, alpha=alpha_pc_fci)\n",
        "            A_rfci = oracle_align_from_skeleton(skel_rfci, ori_rfci, A_true)\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"RFCI\", **directed_metrics(A_true, A_rfci)})\n",
        "            f1_per_target[\"RFCI\"].append((target_names[k], directed_metrics(A_true, A_rfci)[\"f1\"]))\n",
        "\n",
        "            # RCD\n",
        "            A_rcd, conf_rcd = run_rcd(X)\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"RCD\", **directed_metrics(A_true, A_rcd)})\n",
        "            f1_per_target[\"RCD\"].append((target_names[k], directed_metrics(A_true, A_rcd)[\"f1\"]))\n",
        "\n",
        "            # LCDNN (Phases I–III)\n",
        "            lcdnn_out = run_lcdnn(X, lcdnn_cfg)\n",
        "            A_lcdnn = lcdnn_out[\"A_hat\"]\n",
        "            rows.append({\"target\": target_names[k], \"trial\": t, \"method\": \"LCDNN\", **directed_metrics(A_true, A_lcdnn)})\n",
        "            f1_per_target[\"LCDNN\"].append((target_names[k], directed_metrics(A_true, A_lcdnn)[\"f1\"]))\n",
        "\n",
        "            inter_frac, typing_acc = evaluate_typing_accuracy(lcdnn_out[\"conf_pairs\"], lcdnn_out[\"type_labels\"], true_inter)\n",
        "            typing_per_target.append({\n",
        "                \"target\": target_names[k],\n",
        "                \"trial\": t,\n",
        "                \"inter_fraction\": inter_frac,\n",
        "                \"typing_accuracy\": typing_acc,\n",
        "                \"r_hat\": float(lcdnn_out[\"type_info\"][\"r_hat\"]),\n",
        "            })\n",
        "\n",
        "    # -----------------------\n",
        "    # Aggregate main summary\n",
        "    # -----------------------\n",
        "    df_runs = pd.DataFrame(rows)\n",
        "    summary = (\n",
        "        df_runs.groupby(\"method\")[[\"precision\", \"recall\", \"f1\"]]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .sort_values(\"method\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Reorder methods to match the paper\n",
        "    method_order = [\"PC\", \"DirectLiNGAM\", \"NOTEARS\", \"GraN-DAG\", \"FCI\", \"RFCI\", \"RCD\", \"LCDNN\"]\n",
        "    if not run_grandag:\n",
        "        method_order.remove(\"GraN-DAG\")\n",
        "    summary[\"method\"] = pd.Categorical(summary[\"method\"], categories=method_order, ordered=True)\n",
        "    summary = summary.sort_values(\"method\").reset_index(drop=True)\n",
        "\n",
        "    summary_ranked = make_ranked_format_table(summary, [\"precision\", \"recall\", \"f1\"])\n",
        "\n",
        "    # -----------------------\n",
        "    # Table: synth_summary_main\n",
        "    # -----------------------\n",
        "    tex_main = df_to_uai_table_tex(\n",
        "        summary_ranked.rename(columns={\"method\": \"Method\", \"precision\": \"Prec.\", \"recall\": \"Rec.\", \"f1\": \"F1\"}),\n",
        "        caption=\"Synthetic NoNs: directed-edge recovery (macro-averaged over targets and trials). Best is bold; second-best is underlined.\",\n",
        "        label=\"tab:synth_summary_main\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"synth_summary_main.tex\"), \"w\") as f:\n",
        "        f.write(tex_main)\n",
        "\n",
        "    # -----------------------\n",
        "    # Appendix C.1: clustering ARI (Standard GIN vs DL-GIN)\n",
        "    # -----------------------\n",
        "    df_ari = pd.DataFrame(ari_rows)\n",
        "    ari_tbl = df_ari.groupby(\"target\")[[\"Standard GIN\", \"DL-GIN (ours)\"]].mean().reset_index()\n",
        "    ari_tbl.loc[len(ari_tbl)] = [\"Avg.\"] + list(ari_tbl[[\"Standard GIN\", \"DL-GIN (ours)\"]].mean().values)\n",
        "    ari_tbl_fmt = ari_tbl.copy()\n",
        "    for c in [\"Standard GIN\", \"DL-GIN (ours)\"]:\n",
        "        ari_tbl_fmt[c] = ari_tbl[c].apply(lambda x: fmt_num(float(x), 2))\n",
        "    tex_ari = df_to_uai_table_tex(\n",
        "        ari_tbl_fmt.rename(columns={\"target\": \"Target\"}),\n",
        "        caption=\"Synthetic NoNs: clustering quality (ARI) for Standard GIN vs DL-GIN.\",\n",
        "        label=\"tabC:synth_ari\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"synth_ari.tex\"), \"w\") as f:\n",
        "        f.write(tex_ari)\n",
        "\n",
        "    # -----------------------\n",
        "    # Appendix C.2: full structure recovery table (P/R/F1)\n",
        "    # -----------------------\n",
        "    full_tbl = (\n",
        "        df_runs.groupby(\"method\")[[\"precision\", \"recall\", \"f1\"]]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "    )\n",
        "    full_tbl[\"method\"] = pd.Categorical(full_tbl[\"method\"], categories=method_order, ordered=True)\n",
        "    full_tbl = full_tbl.sort_values(\"method\").reset_index(drop=True)\n",
        "    full_tbl_fmt = make_ranked_format_table(full_tbl, [\"precision\", \"recall\", \"f1\"])\n",
        "    tex_full = df_to_uai_table_tex(\n",
        "        full_tbl_fmt.rename(columns={\"method\": \"Method\", \"precision\": \"Precision\", \"recall\": \"Recall\", \"f1\": \"F1\"}),\n",
        "        caption=\"Synthetic NoNs: directed-edge recovery (macro average).\",\n",
        "        label=\"tabC:synth_structure_full\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"synth_structure_full.tex\"), \"w\") as f:\n",
        "        f.write(tex_full)\n",
        "\n",
        "    # -----------------------\n",
        "    # Appendix C.3: per-target F1 for selected methods (FCI, RCD, LCDNN)\n",
        "    # -----------------------\n",
        "    df_f1 = df_runs[df_runs[\"method\"].isin([\"FCI\", \"RCD\", \"LCDNN\"])].copy()\n",
        "    f1_stage = df_f1.groupby([\"method\", \"target\"])[\"f1\"].mean().reset_index()\n",
        "    f1_pivot = f1_stage.pivot(index=\"method\", columns=\"target\", values=\"f1\").reset_index()\n",
        "    # add Avg.\n",
        "    f1_pivot[\"Avg.\"] = f1_pivot[[c for c in f1_pivot.columns if c.startswith(\"N\")]].mean(axis=1)\n",
        "    # format\n",
        "    f1_fmt = f1_pivot.copy()\n",
        "    for c in [c for c in f1_fmt.columns if c != \"method\"]:\n",
        "        f1_fmt[c] = f1_fmt[c].apply(lambda x: fmt_num(float(x), 2))\n",
        "    tex_f1 = df_to_uai_table_tex(\n",
        "        f1_fmt.rename(columns={\"method\": \"Method\"}),\n",
        "        caption=\"Synthetic NoNs: directed-edge F1 per target (selected methods).\",\n",
        "        label=\"tabC:synth_f1_per_target\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"synth_f1_per_target.tex\"), \"w\") as f:\n",
        "        f.write(tex_f1)\n",
        "\n",
        "    # -----------------------\n",
        "    # Appendix C.4: typing table for LCDNN\n",
        "    # -----------------------\n",
        "    df_type = pd.DataFrame(typing_per_target)\n",
        "    type_tbl = df_type.groupby(\"target\")[[\"inter_fraction\", \"typing_accuracy\", \"r_hat\"]].mean().reset_index()\n",
        "    type_tbl.loc[len(type_tbl)] = [\"Avg.\"] + list(type_tbl[[\"inter_fraction\", \"typing_accuracy\", \"r_hat\"]].mean().values)\n",
        "    type_fmt = type_tbl.copy()\n",
        "    type_fmt[\"inter_fraction\"] = type_tbl[\"inter_fraction\"].apply(lambda x: fmt_num(float(x), 2))\n",
        "    type_fmt[\"typing_accuracy\"] = type_tbl[\"typing_accuracy\"].apply(lambda x: fmt_num(float(x), 2))\n",
        "    type_fmt[\"r_hat\"] = type_tbl[\"r_hat\"].apply(lambda x: fmt_num(float(x), 2))\n",
        "    tex_type = df_to_uai_table_tex(\n",
        "        type_fmt.rename(columns={\"target\": \"Target\", \"inter_fraction\": \"Inter fraction\", \"typing_accuracy\": \"Accuracy\", \"r_hat\": r\"$\\hat r$\"}),\n",
        "        caption=\"Synthetic NoNs: typing results for LCDNN (inter-fraction and accuracy).\",\n",
        "        label=\"tabC:synth_typing\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"synth_typing.tex\"), \"w\") as f:\n",
        "        f.write(tex_type)\n",
        "\n",
        "    return {\n",
        "        \"synth_summary_main\": summary,\n",
        "        \"synth_ari\": ari_tbl,\n",
        "        \"synth_structure_full\": full_tbl,\n",
        "        \"synth_f1_per_target\": f1_pivot,\n",
        "        \"synth_typing\": type_tbl,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 11) Optional: causalAssembly benchmark (Section 6.3 + Appendix C.5–C.8)\n",
        "# ============================================================\n",
        "\n",
        "def _causalassembly_load():\n",
        "    from causalassembly import ProductionLineGraph\n",
        "    import requests\n",
        "\n",
        "    data = ProductionLineGraph.get_data()\n",
        "    gt = ProductionLineGraph.get_ground_truth()  # ProductionLineGraph object\n",
        "    return data, gt\n",
        "\n",
        "def _try_get_cells(gt):\n",
        "    # robust access\n",
        "    if hasattr(gt, \"cells\"):\n",
        "        return gt.cells\n",
        "    if hasattr(gt, \"stations\"):\n",
        "        return gt.stations\n",
        "    # fallback: try dict\n",
        "    if isinstance(gt, dict) and \"cells\" in gt:\n",
        "        return gt[\"cells\"]\n",
        "    raise RuntimeError(\"Could not access stage/cell structure from causalAssembly ProductionLineGraph.\")\n",
        "\n",
        "def _to_networkx(gt):\n",
        "    # robust networkx conversion\n",
        "    if hasattr(gt, \"to_networkx\"):\n",
        "        return gt.to_networkx()\n",
        "    if hasattr(gt, \"nx_graph\"):\n",
        "        return gt.nx_graph\n",
        "    # fallback: if gt already is nx graph\n",
        "    try:\n",
        "        import networkx as nx\n",
        "        if isinstance(gt, nx.DiGraph):\n",
        "            return gt\n",
        "    except Exception:\n",
        "        pass\n",
        "    raise RuntimeError(\"Could not convert ground truth to a NetworkX DiGraph.\")\n",
        "\n",
        "def _build_node_to_stage_map(cells) -> Dict[str, str]:\n",
        "    node2stage = {}\n",
        "    # cells may be dict[str, nx_graph or custom]\n",
        "    for stage_name, cell in cells.items():\n",
        "        # try networkx-like\n",
        "        if hasattr(cell, \"nodes\"):\n",
        "            for v in cell.nodes():\n",
        "                node2stage[str(v)] = str(stage_name)\n",
        "        elif isinstance(cell, dict) and \"nodes\" in cell:\n",
        "            for v in cell[\"nodes\"]:\n",
        "                node2stage[str(v)] = str(stage_name)\n",
        "        else:\n",
        "            # try attribute\n",
        "            if hasattr(cell, \"visible_nodes\") or hasattr(cell, \"hidden_nodes\"):\n",
        "                try:\n",
        "                    for v in cell.visible_nodes():\n",
        "                        node2stage[str(v)] = str(stage_name)\n",
        "                    for v in cell.hidden_nodes():\n",
        "                        node2stage[str(v)] = str(stage_name)\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return node2stage\n",
        "\n",
        "def _extract_stage_observed_nodes(stage_name: str, cells, data_cols: List[str]) -> List[str]:\n",
        "    cell = cells[stage_name]\n",
        "    # get candidate nodes from cell\n",
        "    nodes = []\n",
        "    if hasattr(cell, \"nodes\"):\n",
        "        nodes = [str(v) for v in cell.nodes()]\n",
        "    elif hasattr(cell, \"visible_nodes\"):\n",
        "        nodes = [str(v) for v in cell.visible_nodes()]\n",
        "    elif isinstance(cell, dict) and \"nodes\" in cell:\n",
        "        nodes = [str(v) for v in cell[\"nodes\"]]\n",
        "    # intersect with data columns\n",
        "    obs = [v for v in nodes if v in data_cols]\n",
        "    return obs\n",
        "\n",
        "def _ground_truth_within_stage_edges(nxg, obs_nodes: List[str]) -> np.ndarray:\n",
        "    import networkx as nx\n",
        "    p = len(obs_nodes)\n",
        "    idx = {v: i for i, v in enumerate(obs_nodes)}\n",
        "    A = np.zeros((p, p), dtype=int)\n",
        "    for u, v in nxg.edges():\n",
        "        u = str(u); v = str(v)\n",
        "        if u in idx and v in idx:\n",
        "            A[idx[u], idx[v]] = 1\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A\n",
        "\n",
        "def _stage_local_confounded_pairs(nxg, obs_nodes: List[str], hidden_set: Set[str]) -> np.ndarray:\n",
        "    import networkx as nx\n",
        "    p = len(obs_nodes)\n",
        "    idx = {v: i for i, v in enumerate(obs_nodes)}\n",
        "\n",
        "    # precompute ancestors for each node\n",
        "    anc = {}\n",
        "    for v in obs_nodes:\n",
        "        anc[v] = set(str(a) for a in nx.ancestors(nxg, v)) & hidden_set\n",
        "\n",
        "    conf = np.zeros((p, p), dtype=int)\n",
        "    for i in range(p):\n",
        "        for j in range(i + 1, p):\n",
        "            vi, vj = obs_nodes[i], obs_nodes[j]\n",
        "            if len(anc[vi].intersection(anc[vj])) > 0:\n",
        "                conf[i, j] = conf[j, i] = 1\n",
        "    return conf\n",
        "\n",
        "def run_causalassembly_experiments(\n",
        "    lcdnn_cfg: LCDNNConfig,\n",
        "    alpha_pc_fci: float = 0.01,\n",
        "    out_dir: str = \"tables_generated\"\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    ensure_dir(out_dir)\n",
        "    import networkx as nx\n",
        "\n",
        "    data, gt = _causalassembly_load()\n",
        "    cells = _try_get_cells(gt)\n",
        "    nxg = _to_networkx(gt)\n",
        "\n",
        "    # node-to-stage\n",
        "    node2stage = _build_node_to_stage_map(cells)\n",
        "\n",
        "    stage_names = list(cells.keys())\n",
        "    # heuristically keep 5 stages in order if present\n",
        "    stage_names = sorted(stage_names)[:5]\n",
        "\n",
        "    # results collectors\n",
        "    stage_size_rows = []\n",
        "    structure_rows = []\n",
        "    f1_stage_rows = []\n",
        "    span_rows = []\n",
        "    typing_rows = []\n",
        "\n",
        "    for stage_name in tqdm(stage_names, desc=\"causalAssembly stages\"):\n",
        "        obs_nodes = _extract_stage_observed_nodes(stage_name, cells, list(data.columns))\n",
        "        if len(obs_nodes) < 5:\n",
        "            continue\n",
        "\n",
        "        X = standardize(data[obs_nodes].to_numpy(dtype=float))\n",
        "        p = X.shape[1]\n",
        "        # Hidden set: all nodes not in this stage's observed nodes\n",
        "        all_nodes = set(str(v) for v in nxg.nodes())\n",
        "        obs_set = set(obs_nodes)\n",
        "        hidden_set = all_nodes - obs_set\n",
        "\n",
        "        A_true = _ground_truth_within_stage_edges(nxg, obs_nodes)\n",
        "        conf_true = _stage_local_confounded_pairs(nxg, obs_nodes, hidden_set=hidden_set)\n",
        "\n",
        "        # Methods: FCI, RFCI, RCD, LCDNN (as in paper tables)\n",
        "        # FCI\n",
        "        skel_fci, ori_fci, g_fci = run_fci(X, alpha=alpha_pc_fci)\n",
        "        A_fci = oracle_align_from_skeleton(skel_fci, ori_fci, A_true)\n",
        "        met_fci = directed_metrics(A_true, A_fci)\n",
        "        structure_rows.append({\"stage\": stage_name, \"method\": \"FCI\", **met_fci})\n",
        "        f1_stage_rows.append({\"stage\": stage_name, \"method\": \"FCI\", \"f1\": met_fci[\"f1\"]})\n",
        "\n",
        "        # RFCI\n",
        "        skel_rfci, ori_rfci, g_rfci = run_rfci(X, alpha=alpha_pc_fci)\n",
        "        A_rfci = oracle_align_from_skeleton(skel_rfci, ori_rfci, A_true)\n",
        "        met_rfci = directed_metrics(A_true, A_rfci)\n",
        "        structure_rows.append({\"stage\": stage_name, \"method\": \"RFCI\", **met_rfci})\n",
        "        f1_stage_rows.append({\"stage\": stage_name, \"method\": \"RFCI\", \"f1\": met_rfci[\"f1\"]})\n",
        "\n",
        "        # RCD\n",
        "        A_rcd, conf_rcd_pairs = run_rcd(X)\n",
        "        met_rcd = directed_metrics(A_true, A_rcd)\n",
        "        structure_rows.append({\"stage\": stage_name, \"method\": \"RCD\", **met_rcd})\n",
        "        f1_stage_rows.append({\"stage\": stage_name, \"method\": \"RCD\", \"f1\": met_rcd[\"f1\"]})\n",
        "\n",
        "        # LCDNN\n",
        "        lcdnn_out = run_lcdnn(X, lcdnn_cfg)\n",
        "        A_lcdnn = lcdnn_out[\"A_hat\"]\n",
        "        met_lcdnn = directed_metrics(A_true, A_lcdnn)\n",
        "        structure_rows.append({\"stage\": stage_name, \"method\": \"LCDNN\", **met_lcdnn})\n",
        "        f1_stage_rows.append({\"stage\": stage_name, \"method\": \"LCDNN\", \"f1\": met_lcdnn[\"f1\"]})\n",
        "\n",
        "        # Stage sizes + estimated clusters + r_hat\n",
        "        m_hat = len(lcdnn_out[\"clusters_hat\"])\n",
        "        r_hat = float(lcdnn_out[\"type_info\"][\"r_hat\"])\n",
        "        stage_size_rows.append({\"stage\": stage_name, \"p\": p, r\"$\\hat m$\": m_hat, r\"$\\hat r$\": r_hat})\n",
        "\n",
        "        # Latent detection F1 (confounded pairs): compare unordered pairs\n",
        "        def conf_pairs_from_matrix(M):\n",
        "            p = M.shape[0]\n",
        "            pairs = set()\n",
        "            for i in range(p):\n",
        "                for j in range(i + 1, p):\n",
        "                    if M[i, j] == 1:\n",
        "                        pairs.add((i, j))\n",
        "            return pairs\n",
        "\n",
        "        true_pairs = conf_pairs_from_matrix(conf_true)\n",
        "        lcdnn_pairs = set(lcdnn_out[\"conf_pairs\"])\n",
        "        rcd_pairs = conf_rcd_pairs  # already unordered (i,j)\n",
        "\n",
        "        def pair_f1(pred: Set[Tuple[int,int]], true: Set[Tuple[int,int]]) -> float:\n",
        "            tp = len(pred & true)\n",
        "            fp = len(pred - true)\n",
        "            fn = len(true - pred)\n",
        "            prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "            rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "            return 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "        f1_lat_rcd = pair_f1(rcd_pairs, true_pairs)\n",
        "        f1_lat_lcdnn = pair_f1(lcdnn_pairs, true_pairs)\n",
        "\n",
        "        # \"rho\" span alignment: data-driven proxy\n",
        "        # True inter-span proxy: SVD of Cov(R_true, X_out), using X_out = all other observed columns\n",
        "        X_out_cols = [c for c in data.columns if c not in obs_nodes]\n",
        "        X_out = standardize(data[X_out_cols].to_numpy(dtype=float))\n",
        "        # residuals after regressing out *true within-stage parents* (estimated by OLS)\n",
        "        # Build parent sets from A_true\n",
        "        parents_true = [list(np.where(A_true[:, j] == 1)[0]) for j in range(p)]\n",
        "        B_true_fit = refit_linear_sem(X, parents_true)\n",
        "        R_true = compute_residuals_from_B(X, B_true_fit)\n",
        "\n",
        "        # cross-cov (stage residuals vs out-of-stage observables)\n",
        "        Sigma_ro = (R_true.T @ X_out) / max(R_true.shape[0] - 1, 1)\n",
        "        U_true, s_true, _ = np.linalg.svd(Sigma_ro, full_matrices=False)\n",
        "        r_true = estimate_rank_by_relative_threshold(s_true, rel_tol=0.05)\n",
        "        U_true = U_true[:, :r_true] if r_true > 0 else np.zeros((p, 0))\n",
        "\n",
        "        # estimated inter-span from LCDNN typing info (recompute U_hat)\n",
        "        R_hat = compute_residuals_from_B(X, lcdnn_out[\"B_hat\"])\n",
        "        U_hat, r_hat2, _ = estimate_inter_span_from_residuals(R_hat, lcdnn_out[\"clusters_hat\"], max_rank=10)\n",
        "\n",
        "        # subspace alignment rho via principal angles\n",
        "        def subspace_rho(Ua: np.ndarray, Ub: np.ndarray) -> float:\n",
        "            if Ua.size == 0 or Ub.size == 0:\n",
        "                return 0.0\n",
        "            # orthonormalize\n",
        "            Qa, _ = np.linalg.qr(Ua)\n",
        "            Qb, _ = np.linalg.qr(Ub)\n",
        "            M = Qa.T @ Qb\n",
        "            s = np.linalg.svd(M, compute_uv=False)\n",
        "            return float(np.mean(s ** 2))\n",
        "\n",
        "        rho = subspace_rho(U_true, U_hat)\n",
        "\n",
        "        span_rows.append({\n",
        "            \"stage\": stage_name,\n",
        "            \"RCD F1\": f1_lat_rcd,\n",
        "            \"LCDNN F1\": f1_lat_lcdnn,\n",
        "            r\"$\\hat r$\": r_hat,\n",
        "            r\"$\\rho$\": rho\n",
        "        })\n",
        "\n",
        "        if len(true_pairs) > 0:\n",
        "            # predicted labels\n",
        "            pred_labels = lcdnn_out[\"type_labels\"]\n",
        "            correct_inter = 0\n",
        "            false_intra = 0\n",
        "            total_true = 0\n",
        "            for (i, j) in true_pairs:\n",
        "                total_true += 1\n",
        "                pred_inter = (pred_labels.get((i, j), \"intra_or_mixed\") == \"inter\")\n",
        "                if pred_inter:\n",
        "                    correct_inter += 1\n",
        "                else:\n",
        "                    false_intra += 1\n",
        "            inter_acc = correct_inter / total_true\n",
        "            false_intra_rate = false_intra / total_true\n",
        "        else:\n",
        "            inter_acc, false_intra_rate = 0.0, 0.0\n",
        "\n",
        "        typing_rows.append({\n",
        "            \"stage\": stage_name,\n",
        "            \"Inter accuracy\": inter_acc,\n",
        "            \"False intra\": false_intra_rate\n",
        "        })\n",
        "\n",
        "    # -----------------------\n",
        "    # Build and write tables\n",
        "    # -----------------------\n",
        "    df_stage = pd.DataFrame(stage_size_rows)\n",
        "    df_stage_fmt = df_stage.copy()\n",
        "    tex_stage = df_to_uai_table_tex(\n",
        "        df_stage_fmt.rename(columns={\"stage\": \"Stage\", \"p\": r\"$p$\"}),\n",
        "        caption=\"causalAssembly stage-local view: stage sizes and LCDNN estimates.\",\n",
        "        label=\"tabC:causalassembly_stage_sizes\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"causalassembly_stage_sizes.tex\"), \"w\") as f:\n",
        "        f.write(tex_stage)\n",
        "\n",
        "    df_struct = pd.DataFrame(structure_rows)\n",
        "    struct_tbl = df_struct.groupby(\"method\")[[\"precision\", \"recall\", \"f1\"]].mean().reset_index()\n",
        "    order = [\"FCI\", \"RFCI\", \"RCD\", \"LCDNN\"]\n",
        "    struct_tbl[\"method\"] = pd.Categorical(struct_tbl[\"method\"], categories=order, ordered=True)\n",
        "    struct_tbl = struct_tbl.sort_values(\"method\").reset_index(drop=True)\n",
        "    struct_fmt = make_ranked_format_table(struct_tbl, [\"precision\", \"recall\", \"f1\"])\n",
        "    tex_struct = df_to_uai_table_tex(\n",
        "        struct_fmt.rename(columns={\"method\": \"Method\", \"precision\": \"Precision\", \"recall\": \"Recall\", \"f1\": \"F1\"}),\n",
        "        caption=\"causalAssembly stage-local: within-stage directed-edge recovery (macro average).\",\n",
        "        label=\"tabC:causalassembly_structure_macro\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"causalassembly_structure_macro.tex\"), \"w\") as f:\n",
        "        f.write(tex_struct)\n",
        "\n",
        "    df_f1 = pd.DataFrame(f1_stage_rows)\n",
        "    f1_pivot = df_f1.pivot_table(index=\"method\", columns=\"stage\", values=\"f1\", aggfunc=\"mean\").reset_index()\n",
        "    f1_pivot[\"Avg.\"] = f1_pivot[[c for c in f1_pivot.columns if c != \"method\"]].mean(axis=1)\n",
        "    f1_fmt = f1_pivot.copy()\n",
        "    for c in f1_fmt.columns:\n",
        "        if c != \"method\":\n",
        "            f1_fmt[c] = f1_fmt[c].apply(lambda x: fmt_num(float(x), 2))\n",
        "    tex_f1 = df_to_uai_table_tex(\n",
        "        f1_fmt.rename(columns={\"method\": \"Method\"}),\n",
        "        caption=\"causalAssembly stage-local: directed-edge F1 per stage.\",\n",
        "        label=\"tabC:causalassembly_f1_stagewise\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"causalassembly_f1_stagewise.tex\"), \"w\") as f:\n",
        "        f.write(tex_f1)\n",
        "\n",
        "    df_span = pd.DataFrame(span_rows)\n",
        "    span_tbl = df_span.copy()\n",
        "    for c in [\"RCD F1\", \"LCDNN F1\", r\"$\\hat r$\", r\"$\\rho$\"]:\n",
        "        span_tbl[c] = span_tbl[c].apply(lambda x: fmt_num(float(x), 2))\n",
        "    tex_span = df_to_uai_table_tex(\n",
        "        span_tbl.rename(columns={\"stage\": \"Stage\"}),\n",
        "        caption=\"causalAssembly stage-local: latent detection and spillover-span alignment.\",\n",
        "        label=\"tabC:causalassembly_span\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"causalassembly_span.tex\"), \"w\") as f:\n",
        "        f.write(tex_span)\n",
        "\n",
        "    df_typ = pd.DataFrame(typing_rows)\n",
        "    typ_tbl = df_typ.copy()\n",
        "    for c in [\"Inter accuracy\", \"False intra\"]:\n",
        "        typ_tbl[c] = typ_tbl[c].apply(lambda x: fmt_num(float(x), 2))\n",
        "    tex_typ = df_to_uai_table_tex(\n",
        "        typ_tbl.rename(columns={\"stage\": \"Stage\"}),\n",
        "        caption=\"causalAssembly stage-local: typing performance for LCDNN.\",\n",
        "        label=\"tabC:causalassembly_typing\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"causalassembly_typing.tex\"), \"w\") as f:\n",
        "        f.write(tex_typ)\n",
        "\n",
        "    # main summary table in Section 6.3 (macro average)\n",
        "    main_tbl = struct_tbl.copy()\n",
        "    main_tbl_fmt = make_ranked_format_table(main_tbl, [\"precision\", \"recall\", \"f1\"])\n",
        "    tex_main = df_to_uai_table_tex(\n",
        "        main_tbl_fmt.rename(columns={\"method\": \"Method\", \"precision\": \"Prec.\", \"recall\": \"Rec.\", \"f1\": \"F1\"}),\n",
        "        caption=\"causalAssembly stage-local: within-stage directed-edge recovery (macro average).\",\n",
        "        label=\"tab:causalassembly_summary_main\"\n",
        "    )\n",
        "    with open(os.path.join(out_dir, \"causalassembly_summary_main.tex\"), \"w\") as f:\n",
        "        f.write(tex_main)\n",
        "\n",
        "    return {\n",
        "        \"causalassembly_stage_sizes\": df_stage,\n",
        "        \"causalassembly_structure_macro\": struct_tbl,\n",
        "        \"causalassembly_f1_stagewise\": f1_pivot,\n",
        "        \"causalassembly_span\": df_span,\n",
        "        \"causalassembly_typing\": df_typ,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 12) Run everything and write tables\n",
        "# ============================================================\n",
        "RUN_SYNTHETIC = False\n",
        "RUN_CAUSALASSEMBLY = False   # set False if you only want simulations\n",
        "\n",
        "OUT_DIR = \"tables_generated\"\n",
        "ensure_dir(OUT_DIR)\n",
        "\n",
        "set_all_seeds(0)\n",
        "\n",
        "synth_cfg = SynthConfig(\n",
        "    p=30,\n",
        "    m=6,\n",
        "    r_int=3,\n",
        "    n=1000,\n",
        "    edge_prob=0.6,\n",
        "    interface_size=1,\n",
        "    scales=(0.6, 0.9, 1.3, 0.7, 1.0),\n",
        "    seed0=2026\n",
        ")\n",
        "\n",
        "lcdnn_cfg = LCDNNConfig(\n",
        "    alpha_gin=0.05,\n",
        "    dlgin_rank_method=\"noise\",\n",
        "    dlgin_noise_k=1.5,\n",
        "    dlgin_hsic_method=\"perm\",\n",
        "    dlgin_hsic_n_perm=200,\n",
        "    stdgin_rank_method=\"relative\",\n",
        "    stdgin_rel_tol=0.01,\n",
        "    stdgin_hsic_method=\"perm\",\n",
        "    stdgin_hsic_n_perm=200,\n",
        "    rcd_max_explanatory_num=6,\n",
        "    rcd_cor_alpha=0.05,\n",
        "    rcd_ind_alpha=0.05,\n",
        "    rcd_shapiro_alpha=0.05,\n",
        "    ca_do_prune=False,\n",
        "    alpha_type=0.05,\n",
        "    typing_hsic_method=\"perm\",\n",
        "    typing_hsic_n_perm=200,\n",
        "    max_rank=10,\n",
        "    seed=2026,\n",
        ")\n",
        "\n",
        "all_tables = {}\n",
        "\n",
        "if RUN_SYNTHETIC:\n",
        "    synth_tables = run_synthetic_experiments(\n",
        "        synth_cfg=synth_cfg,\n",
        "        lcdnn_cfg=lcdnn_cfg,\n",
        "        n_trials=20,\n",
        "        alpha_pc_fci=0.01,\n",
        "        out_dir=OUT_DIR,\n",
        "        run_grandag=True\n",
        "    )\n",
        "    all_tables.update(synth_tables)\n",
        "\n",
        "if RUN_CAUSALASSEMBLY:\n",
        "    ca_tables = run_causalassembly_experiments(\n",
        "        lcdnn_cfg=lcdnn_cfg,\n",
        "        alpha_pc_fci=0.01,\n",
        "        out_dir=OUT_DIR\n",
        "    )\n",
        "    all_tables.update(ca_tables)\n",
        "\n",
        "print(f\"\\nDone. LaTeX tables written to: ./{OUT_DIR}/\\n\")\n",
        "print(\"Generated files (key ones):\")\n",
        "for fn in sorted(os.listdir(OUT_DIR)):\n",
        "    if fn.endswith(\".tex\"):\n",
        "        print(\" -\", fn)\n",
        "# ============================================================\n",
        "# 12) Diagnostics + simple hyperparameter tuning helpers\n",
        "# ============================================================\n",
        "def _rank_of_crosscov(X: np.ndarray, C: Set[int], tol: float = 1e-6) -> int:\n",
        "    C = sorted(list(C))\n",
        "    R = [i for i in range(X.shape[1]) if i not in C]\n",
        "    if len(C) == 0 or len(R) == 0:\n",
        "        return 0\n",
        "    Sigma = np.cov(X, rowvar=False, ddof=1)\n",
        "    Sigma_cr = Sigma[np.ix_(C, R)]\n",
        "    s = np.linalg.svd(Sigma_cr, compute_uv=False)\n",
        "    if s.size == 0:\n",
        "        return 0\n",
        "    thresh = max(tol, tol * float(s.max()))\n",
        "    return int(np.sum(s > thresh))\n",
        "\n",
        "\n",
        "\n",
        "def _clusters_to_labels(clusters: Iterable[Iterable[int]], p: int) -> np.ndarray:\n",
        "    lab = np.full(int(p), -1, dtype=int)\n",
        "    for ci, C in enumerate(clusters):\n",
        "        for v in C:\n",
        "            lab[int(v)] = int(ci)\n",
        "    if np.any(lab < 0):\n",
        "        missing = np.where(lab < 0)[0]\n",
        "        raise ValueError(f\"_clusters_to_labels: {missing.size} unassigned indices (first few: {missing[:10].tolist()})\")\n",
        "    return lab\n",
        "\n",
        "def diagnose_lcdnn_one_run(\n",
        "    synth_cfg: SynthConfig,\n",
        "    lcdnn_cfg: LCDNNConfig,\n",
        "    *,\n",
        "    trial_seed: int = 0,\n",
        "    target_scale: Optional[float] = None,\n",
        "    verbose: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    base = generate_base_sem(synth_cfg, seed=synth_cfg.seed0 + 1000 * trial_seed)\n",
        "    clusters_true = base[\"clusters\"]\n",
        "    cl_true = _clusters_to_labels(clusters_true, synth_cfg.p)\n",
        "\n",
        "    # Choose scale (default: strongest spillover)\n",
        "    if target_scale is None:\n",
        "        target_scale = float(max(synth_cfg.scales))\n",
        "\n",
        "    sim = simulate_from_sem(\n",
        "        base, synth_cfg,\n",
        "        scale=float(target_scale),\n",
        "        seed=synth_cfg.seed0 + 1000 * trial_seed + 77\n",
        "    )\n",
        "    X = sim[\"X\"]\n",
        "    A_true = sim[\"A_true\"]\n",
        "\n",
        "    # -------- Phase I clustering\n",
        "    clusters_hat = agglomerative_gin_clustering(\n",
        "        X,\n",
        "        alpha_gin=lcdnn_cfg.alpha_gin,\n",
        "        rank_method=lcdnn_cfg.dlgin_rank_method,\n",
        "        rel_tol=lcdnn_cfg.dlgin_rel_tol,\n",
        "        noise_k=lcdnn_cfg.dlgin_noise_k,\n",
        "        hsic_method=lcdnn_cfg.dlgin_hsic_method,\n",
        "        hsic_n_perm=lcdnn_cfg.dlgin_hsic_n_perm,\n",
        "        hsic_max_sigma_points=lcdnn_cfg.dlgin_hsic_max_sigma_points,\n",
        "        seed=lcdnn_cfg.seed,\n",
        "    )\n",
        "    ari = eval_ari(cl_true, clusters_hat)\n",
        "\n",
        "    # -------- Phase II: RCD baseline (no cluster constraints)\n",
        "    try:\n",
        "        A_rcd, _ = run_rcd(X)\n",
        "        met_rcd = directed_metrics(A_true, A_rcd)\n",
        "    except Exception as e:\n",
        "        met_rcd = {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan}\n",
        "        if verbose:\n",
        "            print(\"[diagnose] RCD baseline failed:\", repr(e))\n",
        "\n",
        "    # -------- LCDNN end-to-end (estimated clusters)\n",
        "    out_lcdnn = run_lcdnn(X, lcdnn_cfg)\n",
        "    met_lcdnn = directed_metrics(A_true, out_lcdnn[\"A_hat\"])\n",
        "\n",
        "    # -------- Phase II upper bound: CA-RCD with ORACLE clusters\n",
        "    try:\n",
        "        B_oracle, _ = ca_rcd(\n",
        "            X,\n",
        "            clusters_true,\n",
        "            rcd_max_explanatory_num=lcdnn_cfg.rcd_max_explanatory_num,\n",
        "            rcd_cor_alpha=lcdnn_cfg.rcd_cor_alpha,\n",
        "            rcd_ind_alpha=lcdnn_cfg.rcd_ind_alpha,\n",
        "            rcd_shapiro_alpha=lcdnn_cfg.rcd_shapiro_alpha,\n",
        "            rcd_MLHSICR=lcdnn_cfg.rcd_MLHSICR,\n",
        "            rcd_bw_method=lcdnn_cfg.rcd_bw_method,\n",
        "            rcd_independence=lcdnn_cfg.rcd_independence,\n",
        "            w_threshold=lcdnn_cfg.ca_w_threshold,\n",
        "            do_prune=lcdnn_cfg.ca_do_prune,\n",
        "            prune_alpha=lcdnn_cfg.ca_prune_alpha,\n",
        "            prune_hsic_method=lcdnn_cfg.ca_prune_hsic_method,\n",
        "            prune_hsic_n_perm=lcdnn_cfg.ca_prune_hsic_n_perm,\n",
        "            prune_hsic_max_sigma_points=lcdnn_cfg.ca_prune_hsic_max_sigma_points,\n",
        "            prune_max_rounds=lcdnn_cfg.ca_prune_max_rounds,\n",
        "            seed=lcdnn_cfg.seed,\n",
        "        )\n",
        "        A_oracle = (np.abs(B_oracle) > lcdnn_cfg.edge_threshold).astype(int)\n",
        "        np.fill_diagonal(A_oracle, 0)\n",
        "        met_oracle = directed_metrics(A_true, A_oracle)\n",
        "    except Exception as e:\n",
        "        met_oracle = {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan}\n",
        "        if verbose:\n",
        "            print(\"[diagnose] Oracle CA-RCD failed:\", repr(e))\n",
        "\n",
        "    # -------- Interface-bottleneck check on TRUE clusters\n",
        "    bottleneck = []\n",
        "    for C in clusters_true:\n",
        "        rk = _rank_of_crosscov(X, C, tol=1e-6)\n",
        "        bottleneck.append({\"cluster_size\": len(C), \"rank_crosscov\": rk})\n",
        "\n",
        "    if verbose:\n",
        "        sizes_hat = sorted([len(C) for C in clusters_hat])\n",
        "        sizes_true = sorted([len(C) for C in clusters_true])\n",
        "        print(f\"Target scale = {target_scale:g}\")\n",
        "        print(f\"True #clusters: {len(clusters_true)} sizes: {sizes_true}\")\n",
        "        print(f\"Estimated #clusters: {len(clusters_hat)} sizes: {sizes_hat}\")\n",
        "        print(f\"ARI: {ari:.3f}\")\n",
        "        print(\"Directed metrics (precision/recall/f1):\")\n",
        "        print(\"  RCD baseline:\", met_rcd)\n",
        "        print(\"  LCDNN (est clusters):\", met_lcdnn)\n",
        "        print(\"  Oracle clusters (upper bound):\", met_oracle)\n",
        "        print(\"Interface-bottleneck check (need rank < cluster_size):\")\n",
        "        for b in bottleneck:\n",
        "            ok = b[\"rank_crosscov\"] < b[\"cluster_size\"]\n",
        "            print(f\"  size={b['cluster_size']:2d} rank={b['rank_crosscov']:2d}  {'OK' if ok else 'VIOLATION'}\")\n",
        "\n",
        "    return {\n",
        "        \"ari\": ari,\n",
        "        \"met_rcd\": met_rcd,\n",
        "        \"met_lcdnn\": met_lcdnn,\n",
        "        \"met_oracle\": met_oracle,\n",
        "        \"bottleneck\": bottleneck,\n",
        "        \"clusters_true\": clusters_true,\n",
        "        \"clusters_hat\": clusters_hat,\n",
        "    }\n",
        "\n",
        "\n",
        "def tune_lcdnn_grid(\n",
        "    synth_cfg: SynthConfig,\n",
        "    *,\n",
        "    n_trials: int = 5,\n",
        "    target_scale: Optional[float] = None,\n",
        "    grid: Optional[Dict[str, List[Any]]] = None,\n",
        "    seed_offset: int = 0,\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    if target_scale is None:\n",
        "        target_scale = float(max(synth_cfg.scales))\n",
        "\n",
        "    if grid is None:\n",
        "        grid = {\n",
        "            \"alpha_gin\": [0.05, 0.1, 0.2],\n",
        "            \"dlgin_noise_k\": [1.0, 1.5, 2.0],\n",
        "            \"dlgin_hsic_n_perm\": [200, 500],\n",
        "            \"rcd_max_explanatory_num\": [2, 3, 4],\n",
        "            \"rcd_ind_alpha\": [0.01, 0.05],\n",
        "            \"rcd_cor_alpha\": [0.01, 0.05],\n",
        "            \"ca_w_threshold\": [1e-6, 0.05, 0.1],\n",
        "            \"edge_threshold\": [1e-6, 0.05, 0.1],\n",
        "        }\n",
        "\n",
        "    keys = list(grid.keys())\n",
        "    combos = list(itertools.product(*[grid[k] for k in keys]))\n",
        "\n",
        "    rows = []\n",
        "    for combo in tqdm(combos, desc=\"Tuning grid\"):\n",
        "        params = dict(zip(keys, combo))\n",
        "        cfg = LCDNNConfig(**params)\n",
        "\n",
        "        f1s = []\n",
        "        aris = []\n",
        "        for t in range(n_trials):\n",
        "            base = generate_base_sem(synth_cfg, seed=synth_cfg.seed0 + seed_offset + 1000 * t)\n",
        "            clusters_true = base[\"clusters\"]\n",
        "            cl_true = _clusters_to_labels(clusters_true, synth_cfg.p)\n",
        "\n",
        "            sim = simulate_from_sem(\n",
        "                base, synth_cfg,\n",
        "                scale=float(target_scale),\n",
        "                seed=synth_cfg.seed0 + seed_offset + 1000 * t + 77\n",
        "            )\n",
        "            X = sim[\"X\"]\n",
        "            A_true = sim[\"A_true\"]\n",
        "\n",
        "            out = run_lcdnn(X, cfg)\n",
        "            f1s.append(directed_metrics(A_true, out[\"A_hat\"])[\"f1\"])\n",
        "            aris.append(eval_ari(cl_true, out[\"clusters_hat\"]))\n",
        "\n",
        "        rows.append({**params, \"mean_f1\": float(np.mean(f1s)), \"mean_ari\": float(np.mean(aris))})\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"mean_f1\", \"mean_ari\"], ascending=False).reset_index(drop=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca1c547",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ca1c547",
        "outputId": "01c1526e-db3a-4fbf-f210-256d00aa47bb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric unit tests: OK\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paper-aligned generator + diagnostics + fast verification run\n",
        "# ============================================================\n",
        "\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Set, Dict, Tuple, Optional, Iterable\n",
        "import math, time, json, hashlib\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Patch: enforce paper's max merge union size s_max=12 in Phase I\n",
        "#    (This is a *hyperparameter in your paper*, not an algorithm change.)\n",
        "# ----------------------------\n",
        "def agglomerative_gin_clustering(\n",
        "    X: np.ndarray,\n",
        "    *,\n",
        "    alpha_gin: float,\n",
        "    rank_method: str = \"noise\",\n",
        "    rel_tol: float = 0.3,\n",
        "    noise_k: float = 1.5,\n",
        "    hsic_method: str = \"perm\",\n",
        "    hsic_n_perm: int = 200,\n",
        "    hsic_max_sigma_points: int = 200,\n",
        "    max_merges: Optional[int] = None,\n",
        "    max_pair_checks: Optional[int] = None,\n",
        "    max_union_size: Optional[int] = 12,   # <- paper: s_max = 12\n",
        "    seed: int = 0,\n",
        ") -> List[Set[int]]:\n",
        "    \"\"\"Greedy agglomerative clustering based on DL-GIN, with optional max union size.\"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    p = X.shape[1]\n",
        "    clusters: List[Set[int]] = [{i} for i in range(p)]\n",
        "    merges = 0\n",
        "\n",
        "    while True:\n",
        "        merged = False\n",
        "\n",
        "        clusters_sorted = sorted(clusters, key=lambda s: (len(s), min(s)))\n",
        "\n",
        "        pairs = []\n",
        "        for a in range(len(clusters_sorted)):\n",
        "            for b in range(a + 1, len(clusters_sorted)):\n",
        "                Ca = clusters_sorted[a]\n",
        "                Cb = clusters_sorted[b]\n",
        "                pairs.append((len(Ca) + len(Cb), min(Ca), min(Cb), Ca, Cb))\n",
        "        pairs.sort(key=lambda t: (t[0], t[1], t[2]))\n",
        "\n",
        "        checks = 0\n",
        "        for _, _, _, Ca, Cb in pairs:\n",
        "            C = Ca | Cb\n",
        "            if len(C) == p:\n",
        "                continue  # never merge into a single cluster (R would be empty)\n",
        "            if (max_union_size is not None) and (len(C) > int(max_union_size)):\n",
        "                continue\n",
        "\n",
        "            pval = dl_gin_pvalue(\n",
        "                X,\n",
        "                C,\n",
        "                alpha_hsic=alpha_gin,\n",
        "                rank_method=rank_method,\n",
        "                rel_tol=rel_tol,\n",
        "                noise_k=noise_k,\n",
        "                hsic_method=hsic_method,\n",
        "                hsic_n_perm=hsic_n_perm,\n",
        "                hsic_max_sigma_points=hsic_max_sigma_points,\n",
        "                seed=seed + merges * 10000 + checks,\n",
        "            )\n",
        "            checks += 1\n",
        "            if pval > alpha_gin:\n",
        "                new_clusters = [S for S in clusters if S != Ca and S != Cb]\n",
        "                new_clusters.append(C)\n",
        "                clusters = new_clusters\n",
        "                merged = True\n",
        "                merges += 1\n",
        "                break\n",
        "\n",
        "            if (max_pair_checks is not None) and (checks >= max_pair_checks):\n",
        "                break\n",
        "\n",
        "        if not merged:\n",
        "            break\n",
        "        if (max_merges is not None) and (merges >= max_merges):\n",
        "            break\n",
        "\n",
        "    return [set(sorted(list(c))) for c in clusters]\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Paper-aligned synthetic generator (from your TeX)\n",
        "# ----------------------------\n",
        "@dataclass(frozen=True)\n",
        "class PaperSynthConfig:\n",
        "    T: int = 5\n",
        "    p: int = 48\n",
        "    m: int = 8\n",
        "    k_int: int = 2\n",
        "    r: int = 2\n",
        "    n: int = 2000\n",
        "    edge_prob_cluster: float = 0.25\n",
        "    scales: Tuple[float, float, float, float, float] = (0.20, 0.50, 1.00, 0.50, 0.20)\n",
        "    # coefficient & loading ranges (paper)\n",
        "    w_low: float = 0.3\n",
        "    w_high: float = 0.7\n",
        "    lam_low: float = 0.8\n",
        "    lam_high: float = 1.2\n",
        "    noise_dist: str = \"laplace\"   # paper: Laplace(0,1)\n",
        "\n",
        "def _sign_uniform(rng: np.random.Generator, low: float, high: float, size):\n",
        "    mag = rng.uniform(low, high, size=size)\n",
        "    sgn = rng.choice([-1.0, 1.0], size=size)\n",
        "    return sgn * mag\n",
        "\n",
        "def _draw_noise(rng: np.random.Generator, shape, dist: str):\n",
        "    dist = dist.lower()\n",
        "    if dist == \"laplace\":\n",
        "        return rng.laplace(loc=0.0, scale=1.0, size=shape)\n",
        "    elif dist == \"gaussian\" or dist == \"normal\":\n",
        "        return rng.normal(loc=0.0, scale=1.0, size=shape)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown noise_dist: {dist}\")\n",
        "\n",
        "def _standardize(X: np.ndarray) -> np.ndarray:\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sd = X.std(axis=0, keepdims=True) + 1e-12\n",
        "    return (X - mu) / sd\n",
        "\n",
        "def generate_paper_target(cfg: PaperSynthConfig, *, seed: int, scale: float) -> Dict:\n",
        "    \"\"\"Generate one target dataset N_j with spillover scale `scale` (paper Algorithm).\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    p, m, k_int, r, n = cfg.p, cfg.m, cfg.k_int, cfg.r, cfg.n\n",
        "\n",
        "    # --- partition into equal-size clusters\n",
        "    assert p % m == 0, \"paper uses equal-size clusters\"\n",
        "    cl_size = p // m\n",
        "    perm = rng.permutation(p)\n",
        "    clusters = [perm[i*cl_size:(i+1)*cl_size].tolist() for i in range(m)]\n",
        "    clusters_sets = [set(c) for c in clusters]\n",
        "\n",
        "    cl_id = np.empty(p, dtype=int)\n",
        "    for ci, C in enumerate(clusters_sets):\n",
        "        for v in C:\n",
        "            cl_id[v] = ci\n",
        "\n",
        "    # --- interface sets per cluster\n",
        "    interface_sets: List[Set[int]] = []\n",
        "    interface_all: Set[int] = set()\n",
        "    for C in clusters_sets:\n",
        "        I = set(rng.choice(list(C), size=min(k_int, len(C)), replace=False).tolist())\n",
        "        interface_sets.append(I)\n",
        "        interface_all |= I\n",
        "\n",
        "    # --- local loadings Lambda^{loc}: p x q, q=m (one latent per cluster)\n",
        "    q = m\n",
        "    Lambda = np.zeros((p, q), dtype=float)\n",
        "    for ci, C in enumerate(clusters_sets):\n",
        "        lam_vec = _sign_uniform(rng, cfg.lam_low, cfg.lam_high, size=len(C))\n",
        "        for kk, v in enumerate(sorted(list(C))):\n",
        "            Lambda[v, ci] = lam_vec[kk]\n",
        "\n",
        "    # --- directed structure across clusters: random topo order + Bernoulli edges\n",
        "    topo = rng.permutation(m).tolist()\n",
        "    topo_pos = {c: i for i, c in enumerate(topo)}\n",
        "    B = np.zeros((p, p), dtype=float)  # our convention: B[u,v] = u -> v coefficient\n",
        "\n",
        "    for a in range(m):\n",
        "        for b in range(m):\n",
        "            if a == b:\n",
        "                continue\n",
        "            if topo_pos[a] >= topo_pos[b]:\n",
        "                continue  # respect topo order\n",
        "            if rng.random() >= cfg.edge_prob_cluster:\n",
        "                continue\n",
        "            # place ONE variable-level edge u in C_a -> v in I_b\n",
        "            u = int(rng.choice(clusters[a], size=1)[0])\n",
        "            v = int(rng.choice(list(interface_sets[b]), size=1)[0])\n",
        "            B[u, v] = float(_sign_uniform(rng, cfg.w_low, cfg.w_high, size=1)[0])\n",
        "\n",
        "    # --- (optional) rescale weights (paper says rho(B) <= 0.8)\n",
        "    # For a DAG this is typically already satisfied (eigs are ~0), but we keep the step for faithfulness.\n",
        "    try:\n",
        "        eigs = np.linalg.eigvals(B)\n",
        "        rho = float(np.max(np.abs(eigs)))\n",
        "    except Exception:\n",
        "        rho = 0.0\n",
        "    if rho > 0.8 + 1e-12:\n",
        "        B = B * (0.8 / rho)\n",
        "\n",
        "    A_true = (np.abs(B) > 0).astype(int)\n",
        "    np.fill_diagonal(A_true, 0)\n",
        "\n",
        "    # --- inter-network loadings Gamma^{int}: p x r, only interface rows are nonzero, then scaled by s_j\n",
        "    Gamma = np.zeros((p, r), dtype=float)\n",
        "    for v in interface_all:\n",
        "        Gamma[v, :] = rng.normal(loc=0.0, scale=1.0, size=r)\n",
        "    Gamma = Gamma * float(scale)\n",
        "\n",
        "    # --- draw samples\n",
        "    L_loc = _draw_noise(rng, (n, q), cfg.noise_dist)\n",
        "    L_int = _draw_noise(rng, (n, r), cfg.noise_dist)\n",
        "    E = _draw_noise(rng, (n, p), cfg.noise_dist)\n",
        "\n",
        "    # model in your paper: X = B^T X + Lambda L_loc + Gamma L_int + e\n",
        "    # row-wise simulation: X = (L_loc Lambda^T + L_int Gamma^T + E) (I - B)^{-1}\n",
        "    A = np.linalg.inv(np.eye(p) - B)  # (I - B)^{-1}\n",
        "    S = L_loc @ Lambda.T + L_int @ Gamma.T + E\n",
        "    X = S @ A\n",
        "    X = _standardize(X)\n",
        "\n",
        "    # --- confounding metadata (optional debug)\n",
        "    interface_mask = (np.abs(Gamma).sum(axis=1) > 1e-12).astype(int)\n",
        "    inter_conf = np.outer(interface_mask, interface_mask).astype(int)\n",
        "    np.fill_diagonal(inter_conf, 0)\n",
        "    local_conf = (cl_id.reshape(-1, 1) == cl_id.reshape(1, -1)).astype(int)\n",
        "    np.fill_diagonal(local_conf, 0)\n",
        "    conf_any = ((inter_conf + local_conf) > 0).astype(int)\n",
        "\n",
        "    return dict(\n",
        "        X=X,\n",
        "        B=B,\n",
        "        A_true=A_true,\n",
        "        clusters_true=clusters_sets,\n",
        "        cl_id_true=cl_id,\n",
        "        interface_sets=interface_sets,\n",
        "        interface_mask=interface_mask,\n",
        "        conf_any=conf_any,\n",
        "        Gamma=Gamma,\n",
        "        scale=float(scale),\n",
        "        seed=int(seed),\n",
        "        rho=float(rho),\n",
        "        n_edges=int(A_true.sum()),\n",
        "    )\n",
        "\n",
        "def generate_paper_instance(cfg: PaperSynthConfig, *, seed: int) -> List[Dict]:\n",
        "    \"\"\"Generate one Monte Carlo instance: targets N1..N5.\"\"\"\n",
        "    out = []\n",
        "    for j in range(cfg.T):\n",
        "        out.append(generate_paper_target(cfg, seed=seed + 1000*j, scale=cfg.scales[j]))\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Metric sanity checks\n",
        "# ----------------------------\n",
        "def _metric_unit_tests():\n",
        "    rng = np.random.default_rng(0)\n",
        "    p = 10\n",
        "    A = (rng.random((p,p)) < 0.1).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "\n",
        "    same = directed_metrics(A, A)\n",
        "    assert abs(same[\"precision\"] - 1.0) < 1e-9\n",
        "    assert abs(same[\"recall\"] - 1.0) < 1e-9\n",
        "    assert abs(same[\"f1\"] - 1.0) < 1e-9\n",
        "\n",
        "    # Transpose should usually break directed edges (unless symmetric by chance)\n",
        "    tr = directed_metrics(A, A.T)\n",
        "    if A.sum() > 0 and (A == A.T).all() is False:\n",
        "        # not a strict assert (randomly could be symmetric), but warn if suspicious\n",
        "        if tr[\"f1\"] > 0.5:\n",
        "            print(\"[WARN] metric unit test: A and A.T are unusually similar; ignoring.\")\n",
        "    print(\"Metric unit tests: OK\")\n",
        "\n",
        "_metric_unit_tests()\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Debug helpers\n",
        "# ----------------------------\n",
        "def overlap_counts(A_true: np.ndarray, A_hat: np.ndarray) -> Dict[str,int]:\n",
        "    A_true = (A_true > 0).astype(int)\n",
        "    A_hat = (A_hat > 0).astype(int)\n",
        "    tp = int(np.sum((A_true==1) & (A_hat==1)))\n",
        "    fp = int(np.sum((A_true==0) & (A_hat==1)))\n",
        "    fn = int(np.sum((A_true==1) & (A_hat==0)))\n",
        "    return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"true\": int(A_true.sum()), \"pred\": int(A_hat.sum())}\n",
        "\n",
        "def transpose_diagnostic(A_true: np.ndarray, A_hat: np.ndarray) -> Dict[str, Dict[str, float]]:\n",
        "    m0 = directed_metrics(A_true, A_hat)\n",
        "    m1 = directed_metrics(A_true, A_hat.T)\n",
        "    return {\"as_is\": m0, \"transposed\": m1}\n",
        "\n",
        "def frac_true_edges_inter_confounded(target: Dict) -> float:\n",
        "    A_true = target[\"A_true\"]\n",
        "    interface = target[\"interface_mask\"].astype(bool)\n",
        "    idx = np.argwhere(A_true == 1)\n",
        "    if idx.shape[0] == 0:\n",
        "        return 0.0\n",
        "    cnt = 0\n",
        "    for u,v in idx:\n",
        "        if interface[u] and interface[v]:\n",
        "            cnt += 1\n",
        "    return cnt / idx.shape[0]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Fast verification runner (LCDNN only)\n",
        "# ----------------------------\n",
        "CACHE_DIR = \"cache_lcdnn_paper_debug\"   # set None to disable caching\n",
        "USE_CACHE = True\n",
        "\n",
        "MODE = \"FAST\"     # \"FAST\" or \"PAPER\"\n",
        "TARGETS_TO_RUN = (2,)   # default: only N3 (0-indexed), strongest spillover\n",
        "N_TRIALS = 3\n",
        "\n",
        "set_all_seeds(0)\n",
        "\n",
        "paper_cfg = PaperSynthConfig()\n",
        "\n",
        "if MODE.upper() == \"FAST\":\n",
        "    # Much faster, still paper-like enough to check that scores are not collapsing.\n",
        "    # (If scores look good, switch MODE=\"PAPER\".)\n",
        "    paper_cfg = PaperSynthConfig(n=1000)  # smaller n\n",
        "    hsic_perm = 200\n",
        "else:\n",
        "    hsic_perm = 500\n",
        "\n",
        "# LCDNN hyperparameters (from Table \"LCDNN hyperparameters used in all experiments\")\n",
        "# - alpha_merge = 0.10\n",
        "# - alpha_test  = 0.01 (Phases II-III)\n",
        "# - kappa = 1.2 for nullspace threshold\n",
        "# - HSIC permutations B=500 (PAPER) / lower for FAST\n",
        "lcdnn_cfg = LCDNNConfig(\n",
        "    # Phase I\n",
        "    alpha_gin=0.10,\n",
        "    dlgin_rank_method=\"noise\",\n",
        "    dlgin_noise_k=1.2,\n",
        "    dlgin_hsic_method=\"perm\",\n",
        "    dlgin_hsic_n_perm=hsic_perm,\n",
        "\n",
        "    # Phase II (RCD tests)\n",
        "    rcd_cor_alpha=0.01,\n",
        "    rcd_ind_alpha=0.01,\n",
        "    rcd_shapiro_alpha=0.01,\n",
        "    rcd_max_explanatory_num=min(5, int(math.floor(math.log(paper_cfg.n)))),  # s_cond = min{5, floor(log n)}\n",
        "    ca_w_threshold=0.05,\n",
        "    ca_do_prune=False,\n",
        "\n",
        "    # Phase III (typing)\n",
        "    alpha_type=0.01,\n",
        "    typing_hsic_method=\"perm\",\n",
        "    typing_hsic_n_perm=hsic_perm,\n",
        "    max_rank=10,\n",
        "\n",
        "    # Thresholding\n",
        "    edge_threshold=0.05,\n",
        "\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "def _stable_hash(obj) -> str:\n",
        "    s = json.dumps(obj, sort_keys=True, default=str).encode(\"utf-8\")\n",
        "    return hashlib.md5(s).hexdigest()\n",
        "\n",
        "def _save_clusters(path: Path, clusters: List[Set[int]]) -> None:\n",
        "    payload = [sorted(list(c)) for c in clusters]\n",
        "    path.write_text(json.dumps(payload))\n",
        "\n",
        "def _load_clusters(path: Path) -> List[Set[int]]:\n",
        "    payload = json.loads(path.read_text())\n",
        "    return [set(map(int, c)) for c in payload]\n",
        "\n",
        "def run_trials(cfg: PaperSynthConfig, lcdnn_cfg: LCDNNConfig, n_trials: int, targets_to_run: Iterable[int]) -> pd.DataFrame:\n",
        "    cache = Path(CACHE_DIR) if (CACHE_DIR is not None) else None\n",
        "    if cache is not None:\n",
        "        cache.mkdir(parents=True, exist_ok=True)\n",
        "        (cache / \"sims\").mkdir(exist_ok=True)\n",
        "        (cache / \"outs\").mkdir(exist_ok=True)\n",
        "\n",
        "    rows = []\n",
        "    for trial in range(n_trials):\n",
        "        # ---- generate / load sim\n",
        "        sim_key = _stable_hash({\"cfg\": asdict(cfg), \"trial\": trial})\n",
        "        sim_path = (cache / \"sims\" / f\"{sim_key}.npz\") if cache is not None else None\n",
        "        sim_meta = (cache / \"sims\" / f\"{sim_key}.json\") if cache is not None else None\n",
        "\n",
        "        if USE_CACHE and (sim_path is not None) and sim_path.exists() and sim_meta.exists():\n",
        "            with np.load(sim_path, allow_pickle=True) as z:\n",
        "                # stored as object arrays of dicts\n",
        "                targets = z[\"targets\"].tolist()\n",
        "            # json meta exists for easier inspection; we don't need to parse it\n",
        "        else:\n",
        "            targets = generate_paper_instance(cfg, seed=cfg.T * 10000 + trial * 123)\n",
        "            if sim_path is not None:\n",
        "                np.savez_compressed(sim_path, targets=np.array(targets, dtype=object))\n",
        "                sim_meta.write_text(json.dumps({\"cfg\": asdict(cfg), \"trial\": trial, \"targets\": len(targets)}))\n",
        "\n",
        "        # ---- run lcdnn per selected target\n",
        "        for j in targets_to_run:\n",
        "            tgt = targets[int(j)]\n",
        "            X = tgt[\"X\"]\n",
        "            A_true = tgt[\"A_true\"]\n",
        "            cl_true = tgt[\"cl_id_true\"]\n",
        "\n",
        "            out_key = _stable_hash({\"cfg\": asdict(cfg), \"lcdnn_cfg\": asdict(lcdnn_cfg), \"trial\": trial, \"target\": int(j)})\n",
        "            out_npz = (cache / \"outs\" / f\"{out_key}.npz\") if cache is not None else None\n",
        "            out_clusters = (cache / \"outs\" / f\"{out_key}_clusters.json\") if cache is not None else None\n",
        "\n",
        "            loaded = bool(USE_CACHE and out_npz is not None and out_clusters is not None and out_npz.exists() and out_clusters.exists())\n",
        "\n",
        "            if loaded:\n",
        "                with np.load(out_npz) as z:\n",
        "                    A_hat = z[\"A_hat\"]\n",
        "                    elapsed = float(z[\"elapsed_sec\"][0])\n",
        "                clusters_hat = _load_clusters(out_clusters)\n",
        "            else:\n",
        "                t0 = time.perf_counter()\n",
        "                out = run_lcdnn(X, lcdnn_cfg)\n",
        "                elapsed = time.perf_counter() - t0\n",
        "                A_hat = out[\"A_hat\"]\n",
        "                clusters_hat = out[\"clusters_hat\"]\n",
        "\n",
        "                if out_npz is not None:\n",
        "                    np.savez_compressed(out_npz, A_hat=A_hat, elapsed_sec=np.array([elapsed], dtype=float))\n",
        "                    _save_clusters(out_clusters, clusters_hat)\n",
        "\n",
        "            dm = directed_metrics(A_true, A_hat)\n",
        "            ari = eval_ari(cl_true, clusters_hat)\n",
        "            diag = transpose_diagnostic(A_true, A_hat)\n",
        "            ov = overlap_counts(A_true, A_hat)\n",
        "\n",
        "            rows.append({\n",
        "                \"trial\": int(trial),\n",
        "                \"target\": int(j) + 1,  # 1..5\n",
        "                \"scale\": float(tgt[\"scale\"]),\n",
        "                \"true_edges\": int(tgt[\"n_edges\"]),\n",
        "                \"pred_edges\": int(ov[\"pred\"]),\n",
        "                \"tp\": int(ov[\"tp\"]),\n",
        "                \"fp\": int(ov[\"fp\"]),\n",
        "                \"fn\": int(ov[\"fn\"]),\n",
        "                \"precision\": float(dm[\"precision\"]),\n",
        "                \"recall\": float(dm[\"recall\"]),\n",
        "                \"f1\": float(dm[\"f1\"]),\n",
        "                \"ari\": float(ari),\n",
        "                \"f1_if_transposed\": float(diag[\"transposed\"][\"f1\"]),\n",
        "                \"elapsed_sec\": float(elapsed),\n",
        "                \"inter_confounded_edge_frac\": float(frac_true_edges_inter_confounded(tgt)),\n",
        "                \"loaded_from_cache\": loaded,\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # macro averages across selected targets (and trials)\n",
        "    summary = df[[\"precision\",\"recall\",\"f1\",\"ari\",\"elapsed_sec\"]].mean().to_frame(\"mean\").T\n",
        "    display(df)\n",
        "    display(summary)\n",
        "    return df\n",
        "\n",
        "df = run_trials(paper_cfg, lcdnn_cfg, n_trials=N_TRIALS, targets_to_run=TARGETS_TO_RUN)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Interpretation hints (printed, not asserted)\n",
        "# ----------------------------\n",
        "bad = df[df[\"precision\"] <= 1e-9]\n",
        "if len(bad) > 0:\n",
        "    print(\"\\n[DIAG] Some runs have precision ~0.\")\n",
        "    print(\"Common causes:\")\n",
        "    print(\"  (1) A_true uses the wrong convention (e.g., sign-only instead of abs, or transposed).\")\n",
        "    print(\"  (2) lingam adjacency convention mismatch (check f1_if_transposed; if it is much higher, fix transpose).\")\n",
        "    print(\"  (3) very few true edges in that trial (true_edges small) — single-trial metrics can be 0 by chance.\")\n",
        "    print(\"\\nLook at columns: true_edges, pred_edges, tp/fp/fn, f1_if_transposed, ari.\")\n",
        "\n",
        "hi_transpose = df[df[\"f1_if_transposed\"] > df[\"f1\"] + 0.2]\n",
        "if len(hi_transpose) > 0:\n",
        "    print(\"\\n[DIAG] A_hat.T matches A_true much better than A_hat in some runs.\")\n",
        "    print(\"This strongly suggests a from/to convention mismatch in how B_hat is parsed from lingam.RCD().\")\n",
        "    print(\"Fix: in parse_lingam_rcd_output(), swap the conversion (use adj[from,to] instead of adj[to,from]) or transpose B_hat before thresholding.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}